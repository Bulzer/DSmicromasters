%matplotlib inline   ==> Inline means Incorporated into a body of text rather than placed as a separate section.
Q) diff between :'<matplotlib.figure.Figure at 0x110b9c450>' and ' <matplotlib.text.Text at 0x94f9320>'
    your code snippet should not produce <matplotlib.figure.Figure at 0x110b9c450> but <matplotlib.text.Text at 0x94f9320> (because your last line is printing a title)


1)why Pandas ?
    Pandas are build up on top of NumPy, so most of the NumPy advantages still hold true.
    However, it uniquely enables ingestion and manipulation of heterogeneous data types in an intuitive fashion.
    Pandas also enables combining large data sets using merge and join.
    And it provides a very efficient library for breaking data sets, transforming, and recombining.
    Another great feature Pandas provides is its visualizations.
    Plugged-in data has been simplified in-built functions that come with data frame.
    And descriptive statistics, by using simple function,is another good part of Pandas.This capability really simplifies the exploratory data analysis.
    Additionally, Pandas library handles time-series data effectively via native methods it provides to ingest, transform, and analyze time-series data.
    Other benefits to using Pandas are the ability to take advantage of native methods to handle missing data and data pivoting,easy data sorting, and description capabilities,fast generation of data plots,and Boolean indexing for fast image processing and other masking operations, just to name a few.
    If you're looking for a functionality to perform some data transformation,chances are Pandas already has it.
It provides almost all major data-wrangling capabilities that data scientists need(like import,transform and virtualize data).
2)How it dos this ?
    Pandas achieves this thanks to two data structures.Namely, a)pandas Series and b)pandas DataFrame.
    Pandas Series:
        A series is one one-dimensional array-like object that provides us with many ways to index data.
        Series acts like an ndarray,but it supports many data types like int,string,float,Python objects etc as a part of the array.
        It is a valid argument to most NumPy methods because of its similarities to arrays.
        The axis labels are collectively referred to as the index,and we can get and set values by these index labels.
    So a series is like a fit sized dictionary in this regard.
    we use pandas DataFrame data structure(DS) more compared to seried DS .
    Pandas DataFrame:
        A DataFrame is a 2-D elastic labelled DS that supports heterogeneous data (with labeled axis for rows and columns).
        Arithmetic operations can appear on both row and column labels.
        We can think of it as a container/dictionary for series objects,where each row is a series.
 
 3)Pandas Series:
       #creating a series object,Which is like an numpy array but we can define the index labels,together with the data.
       >>>ser = pd.Series( data= [100, 'foo', 300, 'bar', 500], index= ['tom', 'bob', 'nancy', 'dan', 'eric'])
       >>>print(ser)
       ==> o/p = 
       tom      100
       bob      foo
       nancy    300
       dan      bar
       eric     500
       dtype: object
       #So, instead of indices being zero through four,we have a five-element series with indexes defined as tom, bob, nancy, etc.
       #Although, I defined the data and index using a clear format in this example,we could have skipped data and index equals.
       #i.e we can also write it as 
       >>>ser = pd.Series([100, 'foo', 300, 'bar', 500], ['tom', 'bob', 'nancy', 'dan', 'eric'])
       #Another thing to recognize about the series compared to NumPy array is that the data types can be heterogeneous.
       (i.e in above 'SER'objet we can see that we have values as 100 , 'foo', 'bar' etc i.e int,string etc ==> heteregeneous data)
       >>>print(ser.index)   #==> gives the list of all indices present in that object.
       #We can use any of the indices by using it within rectangular brackets to access data at that location.
       Eg:>>>print(ser['nancy'])  ==> o/p = 300
       Alternatively,we could have used a loc fn(location fn)of the series object to get the value out of the location.
       So instead of ser['nancy'],if I said ser.loc['nancy'], ==> we get same o/p i.e o/p = 300
       #Now, if we wanted to access multiple locations,we have to use one extra sq. bracket i.e 
       >>>print(ser.loc[['nancy','bob','eric']])
       o/p:
       nancy    300
       bob      foo
       eric     500 
       dtype: object
       #Another way to access data at the series location is to use a numeric indices.Here we access elements for three and one in ser.
       >>>print(ser[[4, 3, 1]])
       o/p:
       eric    500
       dan     bar
       bob     foo
       dtype: object
       #A reminder that, like all other array indices,series indices also start by zero.
       #We can use the iloc fn(index location fn) to get only the value without index
       >>>print(ser.iloc[2]) ==> o/p = 300
       #to check if an index is present in an object
       >>>'bob' in ser   #returns true or false
       #lly we can do all the operations on series
       >>>ser * 2 
       ==> o/p :
       tom         200
       bob      foofoo
       nancy       600
       dan      barbar
       eric       1000
       dtype: object
       **Notice that string values also got repeated 2 times
       #calculating the squares
       >>> ser **2
       ==> we get error,coz it dosnt knowhow to evaluate the squares of the strings
       (TypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int')
       ==> instead we can get the locations of the numeric values ang get the squares of those
       >>>ser[['nancy', 'eric']] ** 2
       
 4)DataFrames:
	pandas DataFrame is a 2-dimensional labeled data structure.
	There are many ways to create data frames .We often just read and ingest data into a data frame,
  1)creating a Data frame:
   a)creating a data frame out of a dictionary of series of objects.
	Remember, we are adding another dimension to our data structure,so we need to label each series object.
	==> first we willl create a dictionary and then we create a data frame from that dictionary
	>>>#creating a dictionary
	>>>d = {'one' : pd.Series([100., 200., 300.], index=['apple', 'ball', 'clock']),
	>>>     'two' : pd.Series([111., 222., 333., 4444.], index=['apple', 'ball', 'cerill', 'dancy'])}
	#creating a DF:
	>>>df = pd.DataFrame(d)
	>>>print(df)
	>>>df
	==> O/P: we will a tabular structure i.e llr to a table with rows and columns i.e 
			one     two
		apple   100.0   111.0
		ball    200.0   222.0
		cerill    NaN   333.0
		clock   300.0     NaN
		dancy     NaN  4444.0
	**Notice:1)the values 'NaN'(That means not a number, to indicate there was no value
				for that or that value was indefined for the index in that series.)
		2) check out the difference b/w outputs of >>>df and >>>print(df)
	>>>df.index  	#gives indexes i.e o/p = Index(['apple', 'ball', 'cerill', 'clock', 'dancy'], dtype='object')
	>>>df.columns 	#gives columns i.e o/p = Index(['one', 'two'], dtype='object')
   b)Creating a DF from Dictionary based on conditions
	a)based on row labels
	>>>pd.DataFrame(d, index=['dancy', 'ball', 'apple'])
	==>o/p:
			one	two
		dancy	NaN	4444.0
		ball	200.0	222.0
		apple	100.0	111.0
	b)based on column labels
	i.e I pick the row labels 'dancy', 'ball', 'apple'just like I did in the line above,
		but I'm adding new columns to it.Remember my dictionary had 'one' and 'two' as columns.
		Instead, I'm asking,give me columns 'two' and 'five.'
	>>>pd.DataFrame(d, index=['dancy', 'ball', 'apple'], columns=['two', 'five'])
	==>o/p:
			two	five
		dancy	4444.0	NaN
		ball	222.0	NaN
		apple	111.0	NaN
	==>we selected the label that doesn't exist i.e 'five',so all values in that column will show up as NaN.
   c)We can also create DF from regular Python dictionaries rather than a series.
	a)here we have a data array(list) of 2 dictionaries
	>>>data = [{'alex': 1, 'joe': 2}, {'ema': 5, 'dora': 10, 'alice': 20}]
	>>>type(data)
	#creating a DF from list called data which contains 2 dictionaries(make sure to use pd.DataFrame(case sensitive)
	>>>pd.DataFrame(data)
	o/p :
		alex	alice	dora	ema	joe
	0	1.0	NaN	NaN	NaN	2.0
	1	NaN	20.0	10.0	5.0	NaN
	
	*Note that all the labels in the dictionaries are assigned as columns(column indexex/labels) of DF
	*We can also give separate row labels to DF i.each
	>>>pd.DataFrame(data, index=['orange', 'red'])
	o/p :
			alex	alice	dora	ema	joe
	orange		1.0	NaN	NaN	NaN	2.0
	red		NaN	20.0	10.0	5.0	NaN
	**lly we can select some of the elements from the dictionary as columns to constrain the data set we've worked with.
	>>>pd.DataFrame(data, columns=['joe', 'dora','alice'])
	o/p:
		joe	dora	alice
	0	2.0	NaN	NaN
	1	NaN	10.0	20.0
	>>>#pd.DataFrame(data, index=['joe', 'dora','alice']) (this will throw error but this worked fine when we tried with series dictionaries ! find reason et concept)
	(ValueError: Shape of passed values is (5, 2), indices imply (5, 3))
	>>>pd.DataFrame(data, index=['joe', 'dora']) (but this will give us the o/p)
	o/p:
			alex	alice	dora	ema	joe
	joe		1.0	NaN	NaN	NaN	2.0
	dora		NaN	20.0	10.0	5.0	NaN
	
  2)Basic DF operations(geting data out of DF):
	a)Numerical Operations:
	>>>df['one']  ==> gives the values in the column named 'one' in DF named 'df'(which we have already ceated) and their row labels
	>>>df['three'] = df['one'] * df['two'] ==> adds a column to DF whose values are product of other 2 columns	
	#now print DF ==> we get 3 columns ==> >>>print(df)
	b)Logical operations:
	#we can also have logical operations which results in a boolean value being generated.
	>>>df['flag'] = df['one'] > 250		
	#It's going to give us true for the values that are greater than 250 in column one.
	#And NaN, again, we can't compare that to any number,NaN will be false by nature.
	>>>df
	o/p:
			one		two		three	flag
	apple		100.0		111.0		11100.0	False
	ball		200.0		222.0		44400.0	False
	cerill		NaN		333.0		NaN	False
	clock		300.0		NaN		NaN	True
	dancy		NaN		4444.0		NaN	False
	c)remove or delete data from data frames:
	(i)pop():is used to return the column,it's provided with its I.D.and delete or drop that column from the frame.along with its respective row labels.
	==> we can pop out a column from a DF and assign it to a variable
	>>>three = df.pop('three') ==> this will remove the column from DF 'df' as well.
	(if we disply 'df' ==> we can see that the column 'three' is not present )
	(ii)del()
	>>>del df['two'] ==> removes column 'two' but that removed column will not be stored in memory.
	  *(iii)insert():
	>>>df.insert(2, 'copy_of_one', df['one'])  ==> creates a replica/copy of another column
	(* 2 means insert at colum index no.2,
	'copy_of_one' in the name of the column,
	df['one'] means,replicate column name 'one'
	>>>df
	o/p:
			one		flag	copy_of_one
	apple		100.0		False	100.0
	ball		200.0		False	200.0
	cerill		NaN		False	NaN
	clock		300.0		True	300.0
	dancy		NaN		False	NaN
	>>>df['one_upper_half'] = df['one'][:2]	
	==>we will create a new column by selecting the first two rows of another column.
	(adding a new column named 'one_upper_half' containing first 2 values of column named 'one' in DF named df)
	o/p:
		one	flag	copy_of_one	one_upper_half
	apple	100.0	False	100.0		100.0
	ball	200.0	False	200.0		200.0
	cerill	NaN	False	NaN		NaN
	clock	300.0	True	300.0		NaN
	dancy	NaN	False	NaN		NaN
    
5)Pandas :Data Ingestion:
	In this lesson we'll be focused on importing data into Python.
	One of the biggest advantages of using pandas is its ability to ingest data from a variety of sources
	in a variety of data types and formats.i.e
	a)Files in the csv format can be ingested into Python as Dataframes using the pandas read_csv fn.
	  (Csv is a simple file format used to store tabular data such as a spreadsheet or a database.)
	 input:path to a csv file
	 o/p: pandas DF object containing the contents of the file.
	b)Using the read_json fn in Python pandas we can ingest the structure and contents of a JSON file
	  as a pandas Dataframe or a series data structure.
	 (JSON is a format for structuring data and it's commonly used for communication within web applications.)
	 input:path to a JSON file or a valid SON string
	 o/p: pandas DF or a series object containing the contents
	c)The data in an html document gets stored as a list of pandas DataFrames using the read_html fn.
	 (Html, is a hyper text markup language,and it's a file format used as the basis of every webpage.)
	  input:a URL or a a file or a raw HTML string
	  o/p: a list of pandas DataFrames
	d)The read_sql_query fn in pandas provides us a way to subset and load data from a RDBMS to Python.
	 i.e input1 = sql query
	     input2 = database connection
	     o/p = pandas DF object containing contents of the file
	 lly, we can load a whole relational table using the pandas read_sql_table fn.
	 i.e input1 = name of table in DB
	     input2 = database connection
	     o/p = pandas DF object containing contents of the table
	  Then it will simply show in tabular format,as a pandas DataFrame data structure.
	  (SQL is used to communicate with a database using queries)
	e)there are also other methods to ingest data like :
	 Google big query,SAS files,Excel tables,clip board contents,pickle files etc
	 (find at http://pandas.pydata.org/pandas-docs/stable/api.html#input-output)

Live Code :Data Ingestion:
--------------------------
we will use a movie data set from the MovieLens website.
Q) how to use below commands in jupyter ? 
  >>>!ls ./movielens  (shows the contents of the folder movielens)
  >>>!cat ./movielens/movies.csv (shows thw contents of csv file)
  >>>!cat ./movielens/movies.csv | wc -l ( shows no. of lines in the csv file i.e gives idea about the size of data)
  >>>!head -5 ./movielens/ratings.csv ( displays first 5 lines of csv file(headings+ 4 data lines) i.e gives idea about the data presentin csv)
I am getting error like this==>'ls' is not recognized as an internal or external command,operable program or batch file.

a)Here are the links to the data source and location:
Data Source: MovieLens web site (filename: ml-20m.zip)
Location: https://grouplens.org/datasets/movielens/
CSV file details:
  ratings.csv contains: userId,movieId,rating, timestamp
  tags.csv : userId,movieId, tag, timestamp
  movies.csv : movieId, title, genres 
Using the read_csv function in pandas, we will ingest these three files.
#loading the csv files into a DF
>>>movies = pd.read_csv('./movielens/movies.csv', sep=',')
>>>print(type(movies)) ==> type is of class Dataframe
>>>movies.head() ==> default displays first 5 rows
>>>movies.head(15) #displays top 15 lines of data,gives idea about the data present in csv.
lly loading other csv files(tags and ratings)
>>>#Timestamps(a column in csv files) represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970
>>>tags = pd.read_csv('./movielens/tags.csv', sep=',')
>>>ratings = pd.read_csv('./movielens/ratings.csv', sep=',', parse_dates=['timestamp']) #converting the timestamp in ratings
>>># For current analysis, we will remove timestamp (we will come back to it!)
>>>del ratings['timestamp']
>>>del tags['timestamp']
#Let's review how we can interact with the object using series and DataFrame functions.

SERIES Operations(Ops):
>>>#Extract 0th row: notice that it is infact a Series
>>>row_0 = tags.iloc[0]
>>>type(row_0)  ==> o/p = pandas.core.series.Series
>>>row_0.index #gives the indexes of row_0 which is a series object,we can use these indexes to get the values as well.
eg : >>>row_0['userId'] ==> o/p = 18
>>>row_0.index		==> o/p = Index(['userId', 'movieId', 'tag'], dtype='object')
>>>tags.columns		==> o/p = Index(['userId', 'movieId', 'tag'], dtype='object')
>>>tags.index		==> o/p = RangeIndex(start=0, stop=465564, step=1)

DF Ops:
>>>tags.columns		==> o/p = Index(['userId', 'movieId', 'tag'], dtype='object')
>>>tags.index		==> o/p = RangeIndex(start=0, stop=465564, step=1)
#To extract a series of rows we need to provide an area of indices.
>>>#Extract row 0, 11, 2000 from DataFrame
>>>tags.iloc[ [0,11,2000] ]

6)Pandas:Descriptive Statistics:(using Pandas to generate descriptive stastistics)
-----------------------------------------------------------------------------------
a)describe()
Some basic summary statistics that you should compute for your data set are min,max,mean,
standard deviation etc.	Pandas does this automatically through the describe function.
Looking at these measures will give you an idea of the nature of your data and they 
can tell you if there's something wrong with your data.
syntax ; data_Frame.describe()
Eg: >>>ratings['ratings'].describe()
b)corr()
Correlation or 'corr',is a function for computing Pearson coefficient, can be used to 
explore the dependencies between different variables and the data.	
syntax: data_frame.corr()
It is used to compute pairwise pearson coefficient of columns.
As a side note, a negative correlation score means if X becomes larger, then Y becomes smaller.
And positive correlation means that the two variables are correlated.
Pearson correlation coefficient(PCC)(denoted by ρ (rho)):
  It is a measure of the linear correlation between two variables X and Y.It has a value
  between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation,
  and −1 is total negative linear correlation. It is also referred to as the Pearson's r, 
  Pearson product-moment correlation coefficient (PPMCC) or bivariate correlation.
Definition:
  PCC is the covariance of the two variables divided by the product of their standard deviations. 
  The form of the definition involves a "product moment",that is, the mean (the first moment about
  the origin) of the product of the mean-adjusted random variables,
  hence the modifier product-moment in the name.
formula:
	ρ(x,y) = (cov(X,Y)) / (σ(x) * σ(y))
	where σ = sigma or Standard Deviation(SD)
	      ρ = PCC
	      cov = coveriance
SD:
Standard deviation is a measure of spread or variability in data(how spread out they are from the average)
.A smaller deviation is good when it comes to errors.

Coveriance:
  covariance is a measure of the joint variability of two random variables.
  If the greater values of one variable mainly correspond with the greater values of the other variable,
  and the same holds for the lesser values, i.e.,the variables tend to show similar behavior,the 
  covariance is positive.In the opposite case, when the greater values of one variable mainly correspond
  to the lesser values of the other, i.e., the variables tend to show opposite behavior,the covariance is negative.
  The sign of the covariance therefore shows the tendency in the linear relationship between the variables. 

Some other correlation coefficients are available,like Kendall and Spearman correlations
and Pandas can offer support for those as well.

c)Pandas also offers a number of statistical functions you can perform over the whole data frame,
a part of the data frame, or individual columns.We refer to all these functions
as 'func'.Just replace your favorite statistical operation with it,
like max, min, mode, and median,and you'll find that function in Pandas.
Syntax : fata_Frame.func() (where func = max(), min (), median(), mode() etc.)
Frequently used optional parameters : axis = 0(for rows) or 1 (for columns)
==> mean():
syntax = data_Frame.mean(axis={0 0r 1})
	axis = 0 : Indx
	axis = 1 : columns
	o/p = series or DF with mean values
lly for std() ==> o/p = series or DF with standard deviation values,normalized by N-1

d)Pandas also provides capabilities for checking a condition over a whole data frame or columns with it.
any() :
	o/p = Returns whether any element is true
use: can detect if a cll matches a condition very quickly.
all() :
o/p :Retuens whether all element is true.
Use:can detect if a column/row matches a condition very quickly.

e)some other usefull functions
count()
clip()
round()
rank()

Live Code for Descrioptive stastistics:
#lets focus on the Ratings column of of Ratings dataset/csv
>>>ratings['rating'].describe()
o/p:
count    2.000026e+07
mean     3.525529e+00
std      1.051989e+00
min      5.000000e-01
25%      3.000000e+00 ==> means 25% of the ratings are below/equal to 3.0
50%      3.500000e+00 ==> means 50% of the ratings are below/equal to 3.5
75%      4.000000e+00 ==> means 75% of the ratings are below/equal to 4.0
max      5.000000e+00
Name: rating, dtype: float64

It says that we have a total of more than 2 million ratings(from count) and recorded with a mean of 3.53.
Standard deviation is a measureof spread or variability in your data(how spread out they are from the average).
A smaller deviation is good when it comes to errors.
lly we can do it for whole DF
>>>ratings.describe()
o/p:
	userId		movieId		rating
count	2.000026e+07	2.000026e+07	2.000026e+07
mean	6.904587e+04	9.041567e+03	3.525529e+00
std	4.003863e+04	1.978948e+04	1.051989e+00
min	1.000000e+00	1.000000e+00	5.000000e-01
25%	3.439500e+04	9.020000e+02	3.000000e+00
50%	6.914100e+04	2.167000e+03	3.500000e+00
75%	1.036370e+05	4.770000e+03	4.000000e+00
max	1.384930e+05	1.312620e+05	5.000000e+00

we can calculate separately
>>>ratings['rating'].mean() lly  ratings.mean()
lly for min(),max(),std(),mode() etc

In our data set, since we only have one column of this type,the rating, running the 
correlation function over the data frame won't give us much information unfortunately.
But let's do it just to show how correlation function formats its output.
>>>ratings.corr()
o/p:
	userId		movieId		rating
userId	1.000000	-0.000850	0.001175
movieId	-0.000850	1.000000	0.002606
rating	0.001175	0.002606	1.000000
So as you would remember a negative correlation score,here means those features in our data sets
are inversely correlated, so one go up one goes down,and increase in one means a decrease in other in that sense.
>>>ratings['rating'].corr ==> we will get a error 
TypeError: corr() missing 1 required positional argument: 'other'
I will check if there is any rating above zero.
So, we can do a global, logical comparison over the whole column
resulting in a series object with Boolean values for each row of our data frame.
chech if any value is greater than 5
>>>filter_1 = ratings['rating'] > 5
it's going to give us that series object with Boolean values in it.
So if a value is greater than five,it's going to have true in that row.Else false
>>>print(filter_1)
>>>filter_1.any() o/p: true/false
any() fn will tell us if any of those values in that filter_1 series is true.
In this case we get the false as o/p because there's no rating in this data set
that's more than or greater than five.
lly we can check if all the values are +ve or greater than 0.
>>>filter_2 = ratings['rating'] > 0 ( as we dont have any value below 0 ==. all are true)
>>>filter_2.all() ==> o/p = true (checks if all the values are true)

Okay, now we explored with some statistics on the data set.
Let's move on to data cleaning and we'll first review the data cleaning functions in pandas for that.


7)Pandas: Data Cleaning:
-------------------------
Q)why clean data ?
 Real-world data is messy.It can have problems related to missing values,
 outliers in the data,an invalid data, for instance, negative values for age,
 and in Python we can also have records within DataFrames as NaN or none values.
 Since we get the data downstream,we usually have no or little control
 over how the data is collected.
Q)So, how do we clean?
 We can replace invalid or NaN values with more appropriate values.
 For invalid values again or gaps we can also try to fill in the data
 instead of removing them.Here, a best estimate for a reasonable value
 can be used as a replacement.There can be different techniques
 to find the best estimateand often domain knowledge is required
 to understand what would be that best estimate.
 For example, for a missing age value of an employee,a reasonable 
 value can be estimated based on the employee's length of employment.
 An interpolation of the data values can also be applied to generate estimations
 of those missing values in both series or DF.
 Based on how the exploratory and statistical analysis of the dataset goes,
 we can also think of dropping some of the fields and values
 that are not important to the task.Outliers, for example,
 might be dropped depending on the situation.
 
Functions used for Data Cleaning :
 a)replace():
 Using the replace function we can globally change values in a DataFrame.
 Eg : >>>df=df.replace(9999.0,0) ==> replaces every 9999.0 with 0
 b)fillna():
 Fillna(pronounce as 'Fill N A') method will replace missing values(NaN values) with the
 last known value forward and backward,meaning going up and going down in the column.
 Eg : df = df.fillna(method = 'ffill') or df = df.fillna(method = 'backfill')
 c)dropna()
 dropna function will drop any row or column with a missing value in the DataFrame.
 With the axis zero option(i.e default takes row),which is also the default for dropna,
 any rows with missing values will be taken out of the DataFrame.
 eg: df.dropna(axis=0) or df.dropna() will remove rows with missing/NaN values
     df.dropna(axis=1) will remove columns with missing/NaN values
 d)You can also interpolate values in both series and DataFrame objects.
 The default for interpolate function is a linear interpolation,meaning the method tries
 to fit the values to occur over line using linear polynomials.
 Eg: df.interpolate()
 There are also other methods for interpolation,but we will leave till linear interpolation 
 for this intro class(like polynomial interpolation)
 we can also use Regex for replacement
 
Live Code : Data Cleaning : Handling missing data:
--------------------------------------------------
==> we will use movies.csv file which we have already loaded as movies
>>>movies.shape ==> gives no. of rows and columns
#check if any row has null/NaN values,we will use isnull() fn first and then we'll check if any of those values are true.
This isnull function run on the whole data frame will give us a boolean data frame.
>>>ratings.isnull().any()
o/p:
userId     False
movieId    False
rating     False
dtype: bool
All of them are false ==> we dont have any null values,hence checking in other datasets
>>>tags= tags.dropna()
o/p:
userId     False
movieId    False
tag         True
dtype: bool
==> we have NaN//null values in tag column of Tags DF
>>>tags= tags.dropna() ==> droping the values
confirm by doing tags= tags.dropna() again and checking the shape of tags

Let's stop here and see how we can explore these datasets through simple visualizations in Pandas.

8)Pandas: Data Visualization:
------------------------------
The plot package offers nice visualizations for
 a)bar charts : generated using bar()(i.e data_Frame.plot.bar()),where each column is represented by a different color,
    and turned into a bar that goes until the value in that column.
 b)box plots : using box()(i.e data_Frame.plot.box()),box plots,are a good way of showing data distribution.
    So each box will have minimum and maximum,and medium for columns, if you look at this graph.
 c)histograms : using hist()(i.e data_Frame.plot.hist()),these are another type of graph,show the distribution of data, 
   and it can show skewness,or unusual dispersion between data values.
 d)plot:Additionally using the plot fn(i.e data_frame()) we can create quick line graphs of our data sets.
   Here we see each column in our data frame represented by a different line,
   and those points connected by straight lines.
 lly we can have data_frame.plot.X where X can be any of ==> area()(area plot),bar()(vertical bar plot),
 barh()(horizontal bar plot),   density()(density plot),hexbin()(hexbin plot),
 kde()(kernal density ectimate plot),   line()(line plot),pie()(pie chart),scatter()(scatter plot)
 we can also have data_Frame.plot() --> dataframe plotting accessor and methods...
 ,data_frame.boxplot()-->makes a boxplot of dataframe coloumns or...
 data_frame.hist() --> draw histogram of dataframe's c...
 
Live code : Data Visualization:
--------------------------------
Matplotlib is a plotting library for Python and Pandas leverages matplotlib underneath for its plots.
So if you want Jupyter to plot the graphs inside the notebooks we'll have to tell Jupyter
to plot inline as we see here.Using the below command
>>>%matplotlib inline  #Inline means Incorporated into a body of text rather than placed as a separate section.
Notice the percentage sign before the matplotlib,this is a symbol for a special class of functions
in Jupyter called magic functions.
#plotting a histogram for the column named rating in the datafram names ratings
>>>ratings.hist(column='rating', figsize=(15,10))

9)Pandas:most frequent Operations:
---------------------------------
#selct all the rows where rating > 0
>>>ratings[ratings['rating' ]> 0 ]
#insrting a new column
>>>say we have a DF named 'df' with 3 columns
>>>df['col4']=df['col3']**2
#add an new row
>>>df.loc[10] = [1,2,3,4]
#delete a row
>>>df.drop(df.index[[5]])
#delete a column
>>>del df['col1']
GROUPBY and AGGREGATE:
Say we have a DF named 'df' with columns studentid,phy,che,maths
say we want to get the average marks of students as students can take more than 1 exams 
>>>df.groupby('studentid').mean()

10)Live code : frequent data operations:
-------------------------------------
Let's look at the first few rows of tag column in the table tags.
>>>tags['tag'].head() 			 #gives first few rows of tag column in tags DF(default is 5 rows)
>>>tags['tag'].tail()			#gives the last few rows
>>>tags[['tag','timestamp']].head()	#gives first few rows of mentioned 2 columns
>>>tags.head()				#gives first few rows of tags DF
#get ratings from 1000 to 1010.
>>>ratings[1000:1010] #includes 1000th row and excludes 1010th row
>>>ratings[:10]			#gives first 10 rows
>>>ratings[-10:]		#gives last 10 rows

#count the no. occurances
Value_counts fn will let you find out the count of each unique value occurring in the input.
>>>tag_counts= tags['tag'].value_counts() #stored in the descending order
>>>print(tag_counts[:2])		#gives top 2 occuring words in tag colunm in tags DF
>>>print(tag_counts[-2:])		#gives last 2 occuring words in tag colunm in tags DF
#plot a graph for top 5 occurances
>>>tag_counts[:5].plot(kind='bar',figsize=(15,10))
What was filtering?
Filtering is where you select data that matches your criteria.
Think of it as two steps.
First we need to develop a filter that encodes our criteria.
Then filter will be applied as a mask to our data frame.
*The filter criteria will label each row in our filter as true or false.
#we will create a filter(bascically a series)
>>>is_highly_rated = ratings['rating'] >= 4.0
#applying it as a filter to the ratings table
>>>ratings[is_highly_rated][:5]		#gives first 5 rows where rating is greater than or equal to 4.0
#creating a filter to get the movies whose genres column contains word Animation in the sentence
>>>is_animation = movies['genres'].str.contains('Animation')
#applying
>>>movies[is_animation][:5]
GROUPBY and AGGREGATE:
**a) get 2 columns from ratings and group by column named 'ratings' and count no. of rows grouped in each o/p row
>>>ratings_count = ratings[['movieId','rating']].groupby('rating').count()
ratings_count
lly we can get the mean of the all values in the columns grouped by ratings column
Note:this mean fn will be applied to all mentioned columns except the groupby column
i.e groupby column ideally becomes an index for the data frame
>>>ratings_count = rats[['userId','movieId','rating']].groupby('rating').mean()
#get the avg rating for each movie
>>>average_rating = ratings[['movieId','rating']].groupby('movieId').mean()
>>>average_rating.tail()
>>>average_rating.head()
# count how many ratings are there in our database for each movieID.
>>>movie_count = ratings[['movieId','rating']].groupby('movieId').count()

11)Pandas: Mege DataFrames(similar to Joins in RDBMS)
a)concat:
say if df1 and df2 are 2 dataframes have 4 rows and 4 cols each while 2 col names being same for both
(i)>>>pd.concat([df1,df1]) ==> o/p = df with sum of rows of df1+df1
==> o/p = 8 rows with same columns(i.e 4 columns)
	while indexes will be in format : 0 to 3 for first 4 rows and again 0 to 3 for next 4 rows
(ii)>>>pd.concat([df1,df2]) ==> o/p = DF with sum of rows of df1+df2,while we will be having all the different column names
				from the both if that col in DF2 is not there if df1(say) ==> o/p will have that that column
				for df1 rows with NaN/missing values values lly applies for DF2.
==> o/p = 8 rows and 6 columns(as 2 col nams are same)
	while indexes will be in format: 0 to 3 for first 4 rows of df1 and again 0 to 3 for next 4 rows of df2

**Instead of having Nan/Missing values we can have inner Joins:
(iii)>>>pd.concat([df1,df2],axis = 1 ,join = 'inner')
Note :In the above case, the concatenated DataFrames were stacked vertically.
Here, they are placed next to each other horizontally.
In the horizontal stacking unfortunately,this isn't the perfect merge for our data either,
as the key columns have been duplicated when they were merged into the new DataFrame separately.
==> o/p : 4 rows with separate indexex starting from 0 to 3 and no. of columns = 8 

b)Append:
	An alternative to concat is append.
It behaves similarly to the concat function,but it is a function of the DataFrame itself.
>>>df1.append(df2)
==> o/p = 8 rows and 6 columns(with NaN values)
	while indexes will be again in format: 0 to 3 for first 4 rows of df1 and again 0 to 3 for next 4 rows of df2

The operation which will give us a true combination of these two frames is called merge.
c)Merge:
==> inner join using merge
>>>pd.merge(df1,df2,how = 'inner')
==> o/p = 4 rows and 6 columns and indexes format : 0 to 3
The benefit of using the merge operation,is that it can eliminate the duplicate columns
between the DataFrames it joins.It behaves like concat with inner join except that it removes
duplicate column names.

Live : code:
#we merged both the tag data and the movies data all into one frame.
t = movies.merge(tags, on='movieId', how='inner')
t.head()
Inner Join : The INNER JOIN  selects records that have matching values in both tables.(i.e intersection)
Outer Join : gives all the records(matching records are combined into one + other records from both the table with Nan/missing values)

Combine aggreagation, merging, and filters to get useful analytics:



 


	
	
	
	
	
