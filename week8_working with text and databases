The primary data structure for a relational model is a table,
Notice that a table structure in a DB is very similar to a Python DataFrame.
==>DataFrames can be used to load relational tables(DB tables)
   which are also called relations.
This DB table actually represents a set of tuples(rows).
We were informally calling this a record before
but now we call it a tuple.
Therefore:Table is called as relation.
          Row/record is called as Tuple.
Remember the definition of sets,
it's a collection of distinct elements
of the same type.(remember the word 'distinct' and 'Same type')          
In general many systems/DB's will allow duplicate tuples in their relations(tables)
but mechanisms are provided to prevent duplicate entries.

Note:It is important to understand that join
is one of the most expensive,
that is time and space consuming, operations.
As data becomes larger
and the tables contains hundreds of millions of tuples,
the join operation can easily become a bottleneck
in a larger analytical application.
So, for data science
that involves big data,when we need joins,
it's very important to choose
a suitable data management platform
that makes its operation efficient.

Access a DB with Python:
------------------------
We will use the Iris Database
from Kaggle in SQLite format.
It features 50 iris flowers
and their classification into three species.
https://www.kaggle.com/uciml/iris/
The dataset is called/named database.sqlite,
First let's check that the sqlite database is available and display an error message if the file is not available
(assert checks if the expression is True, otherwise throws AssertionError with the error message string provided):
i.e check if the file is present at our location
>>>import os
>>>data_iris_folder_content = os.listdir("A:/Naveed/DS/micromasters/Week8")
>>>error_message = "Error: sqlite file not available"
>>>assert "database.sqlite" in data_iris_folder_content, error_message
Please note that this error message operation
shows an assertion or a good way of capturing errors
in a program using an assert statement.
Assert statements generally help you locate
or identify bugs in your Python programs,
as a way of creating such built-in tests.
If you do a lot of testing,
you'll actually do a lot of assert statements.
We use the sqlite3 package from the Python standard library to connect to the sqlite database:
SQLite comes with your standard Python,
so it's not a library that you need to go and install separately.
>>>import sqlite3
>>>conn = sqlite3.connect('A:/Naveed/DS/micromasters/Week8/database.sqlite')
Now we have a connection object,
and we'll use this connection object
to get a cursor object.That cursor object
is actually our interface to the database.
A sqlite3.Cursor object is our interface to the database, 
llr to execute method that allows to run any SQL query on our database.
>>>cursor = conn.cursor()
>>>type(cursor)
O/P:sqlite3.Cursor
First of all we can get a list of all the tables saved into the database, 
this is done by reading the column name from the sqlite_master metadata table with:
SELECT name FROM sqlite_master
The output of the execute method is an iterator that can be used
in a for loop to print the value of each row.
>>>for row in cursor.execute("SELECT name FROM sqlite_master"):
>>>    print(row)
O/P:('Iris',)
we only have one table==>so we got that one row.
A shortcut to directly execute the query and gather the results is the `fetchall` method:
==>we can use FETCHALL method instead of loop
>>>cursor.execute("SELECT name FROM sqlite_master").fetchall()
>>>sample_data = cursor.execute("SELECT * FROM Iris LIMIT 20").fetchall()
>>>print(type(sample_data))
==> its of type LIST ==> easy to work,but convert it to DataFrame==> more easy to work
>>>sample_data
>>>[row[0] for row in cursor.description]
(checking the colum names of the list)
Note: This way of finding the available tables in a database is specific to sqlite, 
other databases like MySQL or PostgreSQL have different syntax.
Then we can execute standard SQL query on the database, 
SQL is a language designed to interact with data stored in a relational database. 
It has a standard specification, therefore the commands below work on any database.

If you need to connect to another database, you would use another package instead of sqlite3,
Eg:
MySQL Connector for MySQL:https://dev.mysql.com/doc/connector-python/en/
Psycopg for PostgreSQL :http://initd.org/psycopg/docs/install.html
pymssql for Microsoft MS SQL :http://pymssql.org/en/stable/
then you would connect to the database using specific host, port and authentication credentials but then you could execute the same exact SQL statements.
>>>import pandas as pd
>>>iris_data = pd.read_sql_query("SELECT * FROM Iris", conn)
(reading the O/P into a pandas dataframe)
pandas.read_sql_query takes a SQL query and a connection object
and imports the data into a DataFrame, also keeping the same data types of the database columns
>>>iris_data.head()
>>>iris_data.dtypes
(gives datatypes of columns)
However, sqlite3 is extremely useful for downselecting data before importing them in pandas.
For example you might have 1 TB of data in a table stored in a database on a server machine.
You are interested in working on a subset of the data based on some criterion, 
unfortunately it would be impossible to first load data into pandas and then filter them,
therefore we should tell the database to perform the filtering and just load into pandas the downsized dataset.
>>>iris_setosa_data = pd.read_sql_query("SELECT * FROM Iris WHERE Species == 'Iris-setosa'", conn)
we are selecting iris_setosa rather than rather than all three species.
>>>iris_setosa_data
>>>print(iris_setosa_data.shape)
>>>print(iris_data.shape)
------------------------------------------------------------------------------
NLP:NLTK:
-------------------------------------------------------------------------------
Natural Language Processing, or shortly NLP,is a data science term used to refer
to the interaction of computers,and natural language humans use.
NLP techniques are applied in speech recognition engines,
like Siri, Google Now, or Alexa.
These engines are designed to learn what and how a
human talks over time,and constantly improve their accuracy.
Similarly, automatic translators,like Google Translate or Facebook
automatic translation of statuses use NLP, using some recent very effective
neural network based techniques that take not only words and phrases into account,
but also the context, by looking at the word surrounding
the text they are translating.

NLTK is the most popular Python package for NLP.
It is an open source library that provides modules
for importing, cleaning, pre-processing text data,
in human language,
and then apply computational linguistics algorithms,
or machine learning algorithms, like sentiment analysis,
to these datasets.
It also provides over 50 datasets to start working with,
including the movie database we will be using
in our example notebook.
Note:We can use NLTK download function to download these datasets.

Code:

It also includes many easy-to-use datasets in the `nltk.corpus` package, 
we can download for example the `movie_reviews` package using the `nltk.download` function:
>>>import nltk
>>>nltk.download("movie_reviews")
o/p:true (i.e dataset is downloaded)
You can also list and download other datasets interactively just typing:
nltk.download() in the Jupyter Notebook.
***Once the data have been downloaded, we can import them from `nltk.corpus`.
>>>from nltk.corpus import movie_reviews
(importing the movie reviews we downloaded)
The FILE ID's `fileids` method provided by all the datasets in `nltk.corpus` 
gives access to a list of all the files available.
In particular in the movie_reviews dataset we have 2000 text files,
each of them is a review of a movie, and they are already split in a 
`neg` folder for the negative reviews and a `pos` folder for the positive reviews:
>>>len(movie_reviews.fileids())
o/p:2000
NL techniques depends on large amounts of text
or other linguistics data.
These digital collections are called corpora all together.
Another word you will hear is a corpus,
which is the singular form of corpora.
Therefore:corpus(plural corpora) is a collection of text in 
digital form ,assembled for text processing.
(NLTK provides a download interface to pre-processed text datasets)
After importing nltk, we were able to interact
with a download interface for all of these datasets
using nltk download.
The movie reviews corpus,
was downloaded in your home folder.
Now,as we have imported this dataset
and the dataset has 2000 records(1000 +ve reviews and 1000 0ve reviews)
>>>movie_reviews.fileids()[:5]
(checking the first 5 files in the corpus)
o/p:
['neg/cv000_29416.txt',
 'neg/cv001_19502.txt',
 'neg/cv002_17424.txt',
 'neg/cv003_12683.txt',
 'neg/cv004_12641.txt']
>>>movie_reviews.fileids()[-5:]
(checking the last 5 fils)
O/P:
['pos/cv995_21821.txt',
 'pos/cv996_11592.txt',
 'pos/cv997_5046.txt',
 'pos/cv998_14111.txt',
 'pos/cv999_13106.txt']
 ==> we can see that the files are placed in folders/folders 'neg' and 'pos'.

`fileids` can also filter the available files based on their category, 
which is the name of the subfolders they are located in. 
Therefore we can have lists of positive and negative reviews separately.
>>>negative_fileids = movie_reviews.fileids('neg')
>>>positive_fileids = movie_reviews.fileids('pos')
 (as we can see the o/p's are Lists(due to sq. braces when we displayes top 5 and last 5 files))
 ==> we can use 'len' fn
 i.e >>>len(negative_fileids), len(positive_fileids)
 o/p:1000,1000
 Lets review a file of this data set
 ==> using 'raw' method to inspect a single file
 >>>print(movie_reviews.raw(fileids=positive_fileids[0]))
 Output is a lot of text data,which is the indetail review of the movie.
 What do we do of this text ???
 ==>The first step in Natural Language processing is generally 
 to split the text into words, this process might appear simple
 but it is very tedious to handle all corner cases,
What are corner cases?
They include inconsistent use of punctuation,
or contractions, or shortened versions of words.
They can also be hyphenated words
that include characters like the
'New York-based' example here.
How do we Tokenize such cases?
Nltk offers libraries to remedy these challenges.
we will first use a simple white space based Tokenizer.
Then, we will learn how to do it better and easier
using nltk.

==> tokenizing:
simple white space based Tokenizer.
==> use string-splitter function in Python for this
Say we below text:
>>>romeo_text = """Why then, O brawling love! O loving hate!
>>>   O any thing, of nothing first create!
>>>   O heavy lightness, serious vanity,
>>>   Misshapen chaos of well-seeming forms,
>>>   Feather of lead, bright smoke, cold fire, sick health,
>>>   Still-waking sleep, that is not what it is!
>>>   This love feel I, that feel no love in this."""
>>>romeo_text.split()
==> we see that punctuations like exclamations,periods,etc
are still with the words
==>`nltk` has a sophisticated word tokenizer trained on English
named `punkt`, to remove these punctuations
we first have to download its parameters: 
>>>nltk.download("punkt")
Then we can use the `word_tokenize` function to properly tokenize
this text, compare to the whitespace splitting we used above.
>>>romeo_words = nltk.word_tokenize(romeo_text)
>>>romeo_words
==> we can see that the words are tokenized properly now.
i.e say we have 'then.'
==> we have 2 tokes 1st is 'then' and 2nd is '.' using punkt
The good news is all corpora in nltk
already provides a way to generate Tokenized words
for each data file.
==>movie_reviews corpus already has direct access to tokenized text with the words method
>>>movie_reviews.words(fileids=positive_fileids[0])

==>creating Bag of words model:
Bag of words: text as unordered collection of words.
Bag-of-words model is a very simple representation
of a body of text as a loose set of words.
It flattens any text into an unordered collection of words.
Although it disregards the sentence structure
associated to the words, this simple technique
is pretty useful to identify a topic or sentiment in text,
like if a product review has
a negative or positive sentiment,
or what a body of text talks about.
We can use the words in a feature matrix
where each word is a column, and each text body
or review in our movie example,
is a row that has boolean data values.
A cell in the review row gets assigned true
if the word appears in the review,
and false if it doesn't.
From the bag-of-words model
we can build features to be used by a classifier
and here we assume that each word is a feature
that can either be true or false.
We implement this in Python as a dictionary,
where each word in a sentence we associate with true,
and if a word is missing, that would be the same
as assigning false

before further analysis it is often practice to filter our stopwords
and maybe even the punctuations from the bag-of-words.
Stopwords are words like the, that, and is,
which occur a lot but don't have a big significance
in identifying the context of the text being processed.
>>>{word:True for word in romeo_words}
(assigning True for each word/token in romeo words)
So we'll have, as you see here, a dictionary
where each word in romeo_words are assigned true,
and there are no false values
because we are not assigning false for anything.
>>>type(_)
o/p: dict
You would remember taht underscore(_) here is the last output
that goes into the standard out.
So if you had assigned that dictionary to a variable,
you need to write the name
of that variable within this type.
(i.e we can also do >>>dict1={word:True for word in romeo_words}
                    >>>type(dict1)
)
Lets generalize it by moving this code into a python function.
>>>def build_bag_of_words_features(words):
>>>    return {word:True for word in words}
==> fn name = build_bag_of_words_features
   this will accept a set of words and return a dictionary for it.
So let's run the function and see if we get that same output
>>>build_bag_of_words_features(romeo_words)
#removing the stopwords by downloading the stopword nltk corpus for english stopwords
>>>nltk.download("stopwords")
importing string class
>>>import string
these are the punctuations present in string class
>>>string.punctuation
O/P:'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
==> we will now remove the stopwords and the punctuations present above.
==> capturing the useless words
>>>useless_words = nltk.corpus.stopwords.words("english") + list(string.punctuation)
>>>#useless_words
>>>#type(useless_words)
Now we will actually update our bag-of-words function
that we built with an if statement
to check if the word exists in useless_words
and skip that word if it does.
>>>def build_bag_of_words_features_filtered(words):
>>>    return {
>>>        word:1 for word in words \
>>>        if not word in useless_words}
==>using the sam sub fn 'word' as before
   replaced True with '1' (since True is 1 and False is 0)
   i.e for each true word,if the word is not present in useless words==> add it to the dictionary)
>>>build_bag_of_words_features_filtered(romeo_words)   
   
==> Plotting Frequency of Words:
Generally we use plotting to compare 2 datasets.
Using the `.words()` function with no argument we 
can extract the words from the entire dataset.
>>>all_words = movie_reviews.words()
after extracting the words...lets count them.
>>>len(all_words)/1e6
O/P:1.58382
==> we have around 1.6 million words.
Now lets count the words after filtering stopwords.
>>>filtered_words = [word for word in movie_reviews.words() if not word in useless_words]
#>>>type(filtered_words)
>>>len(filtered_words)/1e6
O/P:0.710579 i.e .7 million
The `collection` package of the standard library contains 
a `Counter` class that is handy for counting frequencies 
of words in our list.
==>Let's create a counter object using these filtered words.
we'll start by importing counter from collections.
And we'll give counter this filtered words.
And turn that into a counter object.
>>>from collections import Counter
>>>word_counter = Counter(filtered_words)
Once we have this word_counter object successfully ran,
We can use any function the counter class provides us,
or the properties of this class.
==>we have a method called `most_common()` from the 
Counter class to get the words with the higher count.
>>>most_common_words = word_counter.most_common()[:10]
>>>most_common_words
Now we would like to have a visualization of this using matplotlib.
First we want to use the Jupyter magic function
%matplotlib inline
To setup the Notebook to show the plot embedded 
into the Jupyter Notebook page, you can also test:
%matplotlib notebook
for a more interactive plotting interface which 
however is not as well supported on all platforms and browsers.
>>>%matplotlib notebook
#>>>%matplotlib inline
>>>import matplotlib.pyplot as plt
We need to sort this list,
before we can plot it actually.

We can sort the word counts and plot their values
on Logarithmic axes to check the shape of the distribution. 
This visualization is particularly useful if comparing 2 
or more datasets, a flatter distribution indicates a large
vocabulary while a peaked distribution a restricted vocabulary
often due to a focused topic or specialized language.

==>we had sorted word_counter and the values in it.
And we assigned that to a list called sorted_word_counts.
We'll give those sorted word counts to 
the loglog fn to create that logarithmic plot.
>>>sorted_word_counts = sorted(list(word_counter.values()), reverse=True)
>>>plt.loglog(sorted_word_counts)
>>>plt.ylabel("Freq")
>>>plt.xlabel("Word Rank");
We can also plot the histogram of sorted word counts,
which displays how many words have a count,
in a specific range.
Not per word but the words are bent
to be together, if they have a similar count.
Since we have many words with low frequencies,
in our corpus.
>>>plt.hist(sorted_word_counts, bins=50);
we can see that the distribution is highly peaked at low counts
(since we have many words whose frequency is less), i.e. most of the words
appear with a low count, so we better display it on 
semilogarithmic axes to inspect the tail of the distribution.
>>>plt.hist(sorted_word_counts, bins=50, log=True);

Sentiment Analysis:
---------------------------
We now use the bag-of-words model we created 
in a classifier for Sentiment Analysis of movie reviews.
what is Sentiment Analysis?
The term refers to the activity of identifying attitude
or emotion encoded in a body of text,
like a product review(+ve or -ve) or work of literature.
Classification using machine learning
is a technique used for sentiment models.
As you would remember, classification is a
supervised activity and requires labels
from a ground truth data.
This is where we will take advantage of
bag-of-words and a curated negative
and positive reviews we downloaded.
We will use our previously implemented bag-of-words function
to create a positive or negative label
for each review bag-of-words.
We will use Naive Bayes Classifier for this task.

Naive Bayesclasifier:
is a very simple classifier with
a probabilistic approach to classification.
What this means is that the relationships between
the input features and the class labels
is expressed as probabilities.
So, given the input features, for example,
the probability for each class is estimated.
The class with the highest probability
then determines the label for the sample.
==>Naive bayes is a simple classifier
based on conditional probability.
-->In training phase,it detects the probability that
each feature(word) appears in a category(+ve or -ve).
-->Once Trained,it collects the 'votes' for all words
in the new review and finds the most probable label.

**We will use Naive Bayes from nltk this time, not scikit-learn.
==>Training a Classifier for Sentiment Analysis:
Using our `build_bag_of_words_features` fn we can 
build separately the negative and positive features.
Basically for each of the 1000 negative and for the 
1000 positive review, we create one dictionary of the
words and we associate the label "neg" and "pos" to it.
(We are lucky that the database is curated
to separate positive and negative reviews.
So we will use this as ground truth data
and build two dictionaries as a bag-of-words
for positive and negative reviews.)
>>>negative_features = [
>>>    (build_bag_of_words_features_filtered(movie_reviews.words(fileids=[f])), 'neg') \
>>>    for f in negative_fileids
>>>]
we can print a samoke record and test
>>>print(negative_features[3])
lly for positive features
>>>positive_features = [
>>>    (build_bag_of_words_features_filtered(movie_reviews.words(fileids=[f])), 'pos') \
>>>    for f in positive_fileids
>>>]
>>>print(positive_features[6])
Now we have our 2 features and remember that
each of these feature has 1000 labels/records in it.
(1000 records each for +ve lable and -ve labels respectively)
>>>from nltk.classify import NaiveBayesClassifier
we are doing 80-20 split
When we provide the first 800 rows
in each feature, it's 80%, right, 1,000 times 80%.
So we'll store that number, 800,
in a variable called split.
And we'll use that split to slice the first 800 for training
and the remaining 200 for testing later on.
>>>split = 800
==>Now We are using the Naive Bayes Classifier
to train it, with the first 800 positive features
and the first 800 negative features.
>>>sentiment_classifier = NaiveBayesClassifier.train(positive_features[:split]+negative_features[:split])
(Remember they had labels pos and neg.)
we'll call it Sentiment Classifier.
We'll now check the accuracy of the model we built.
using the classification's accuracy fn,
it's the utility of the classification.
Lets check after training what is the accuracy on the training set,
i.e. the same data used for training, we expect this to be a very 
high number because the algorithm already "saw" those data. 
Accuracy is the fraction of the data that is classified correctly,
we can turn it into percent.
>>>nltk.classify.util.accuracy(sentiment_classifier, positive_features[:split]+negative_features[:split])*100
O/p: 98.065
==> preety good as expected
Now lets check how the model behave on the remaining 20 %
>>>nltk.classify.util.accuracy(sentiment_classifier, positive_features[split:]+negative_features[split:])*100
O/p:71.75
===>Accuracy here is around 70% which is pretty good for
such a simple model(we can further improve our model)if we 
consider that the estimated accuracy for a person is about 80%.
There if there were a human instead of 
machine he would have predicted 80% correctly.
We can finally print the most informative features,
i.e. the words that mostly identify a +ve or -ve review.

Remember, we had a large vocabulary and
the Sentiment Classifier used all the words,
but which of those words gave us this highish accuracy?
==> we have a fn for that as well to tell which 
words were most informative in the dataset.
>>>sentiment_classifier.show_most_informative_features()
O/p:
Most Informative Features
       outstanding = 1                 pos : neg    =     13.9 : 1.0
         insulting = 1                 neg : pos    =     13.7 : 1.0
        vulnerable = 1                 pos : neg    =     13.0 : 1.0
         ludicrous = 1                 neg : pos    =     12.6 : 1.0
       uninvolving = 1                 neg : pos    =     12.3 : 1.0
            avoids = 1                 pos : neg    =     11.7 : 1.0
        astounding = 1                 pos : neg    =     11.7 : 1.0
       fascination = 1                 pos : neg    =     11.0 : 1.0
         affecting = 1                 pos : neg    =     10.3 : 1.0
            darker = 1                 pos : neg    =     10.3 : 1.0
==> words like outstanding,insulting etc had more effect on our output

Other uses of NLP:
1)Estimating Oscar Winner/;
A company (I do not remember which) claims to 
have predicted the winner at the Oscars 2016 
(The Revenant) using only reviews posted by the
online community on popular movie review websites. 
is a Prime example of NLP.

2)Using sentiment evaluation to target individuals
that may have mental health issues.:
Sentiment evaluation of social media could be used
to target relevant advertising/websites to people 
with mental health issues, e.g. people that repeatedly 
post sad statuses could be targeted with a helpline advert.
This principle is already being applied with targeted 
marketing based on keyword searches, so why not use it
for a good cause as well.

3)Predict a natural disaster for insurers company.

------------------------------------------------------------------------------------
Accessing Twitter API:
---------------------------




Q)What is the correct way to show the last 2 files using movie_reviews.fileids() where movie_reviews is a downloaded dataset?
A)movie_reviews.fileid()[-2:]
B)movie_reviews.fileids([-2:])
C)movie_reviews.fileids()[:-2]
D)movie_reviews.fileids([:-2])
Ans: (A) 
Q)True or False: We can use ‘1’ and ‘True’ interchangeably.
a)True
b)False
Ans: A (get it confirmed from someone once)
Q)What is the benefit of a log graph over a graph that has not modified the scales?
a)Log scales allow a large range of values that are compressed into one point of the graph to be shown. 83%
b)Log scales are significantly better in representing every type of data compared to unmodified scale graphs. 14%
c)There is no outright advantage to converting a graph into a log scale. 3%
Ans: A (get it confirmed from someone once)

