The primary data structure for a relational model is a table,
Notice that a table structure in a DB is very similar to a Python DataFrame.
==>DataFrames can be used to load relational tables(DB tables)
   which are also called relations.
This DB table actually represents a set of tuples(rows).
We were informally calling this a record before
but now we call it a tuple.
Therefore:Table is called as relation.
          Row/record is called as Tuple.
Remember the definition of sets,
it's a collection of distinct elements
of the same type.(remember the word 'distinct' and 'Same type')          
In general many systems/DB's will allow duplicate tuples in their relations(tables)
but mechanisms are provided to prevent duplicate entries.

Note:It is important to understand that join
is one of the most expensive,
that is time and space consuming, operations.
As data becomes larger
and the tables contains hundreds of millions of tuples,
the join operation can easily become a bottleneck
in a larger analytical application.
So, for data science
that involves big data,when we need joins,
it's very important to choose
a suitable data management platform
that makes its operation efficient.

Access a DB with Python:
------------------------
We will use the Iris Database
from Kaggle in SQLite format.
It features 50 iris flowers
and their classification into three species.
https://www.kaggle.com/uciml/iris/
The dataset is called/named database.sqlite,
First let's check that the sqlite database is available and display an error message if the file is not available
(assert checks if the expression is True, otherwise throws AssertionError with the error message string provided):
i.e check if the file is present at our location
>>>import os
>>>data_iris_folder_content = os.listdir("A:/Naveed/DS/micromasters/Week8")
>>>error_message = "Error: sqlite file not available"
>>>assert "database.sqlite" in data_iris_folder_content, error_message
Please note that this error message operation
shows an assertion or a good way of capturing errors
in a program using an assert statement.
Assert statements generally help you locate
or identify bugs in your Python programs,
as a way of creating such built-in tests.
If you do a lot of testing,
you'll actually do a lot of assert statements.
We use the sqlite3 package from the Python standard library to connect to the sqlite database:
SQLite comes with your standard Python,
so it's not a library that you need to go and install separately.
>>>import sqlite3
>>>conn = sqlite3.connect('A:/Naveed/DS/micromasters/Week8/database.sqlite')
Now we have a connection object,
and we'll use this connection object
to get a cursor object.That cursor object
is actually our interface to the database.
A sqlite3.Cursor object is our interface to the database, 
llr to execute method that allows to run any SQL query on our database.
>>>cursor = conn.cursor()
>>>type(cursor)
O/P:sqlite3.Cursor
First of all we can get a list of all the tables saved into the database, 
this is done by reading the column name from the sqlite_master metadata table with:
SELECT name FROM sqlite_master
The output of the execute method is an iterator that can be used
in a for loop to print the value of each row.
>>>for row in cursor.execute("SELECT name FROM sqlite_master"):
>>>    print(row)
O/P:('Iris',)
we only have one table==>so we got that one row.
A shortcut to directly execute the query and gather the results is the `fetchall` method:
==>we can use FETCHALL method instead of loop
>>>cursor.execute("SELECT name FROM sqlite_master").fetchall()
>>>sample_data = cursor.execute("SELECT * FROM Iris LIMIT 20").fetchall()
>>>print(type(sample_data))
==> its of type LIST ==> easy to work,but convert it to DataFrame==> more easy to work
>>>sample_data
>>>[row[0] for row in cursor.description]
(checking the colum names of the list)
Note: This way of finding the available tables in a database is specific to sqlite, 
other databases like MySQL or PostgreSQL have different syntax.
Then we can execute standard SQL query on the database, 
SQL is a language designed to interact with data stored in a relational database. 
It has a standard specification, therefore the commands below work on any database.

If you need to connect to another database, you would use another package instead of sqlite3,
Eg:
MySQL Connector for MySQL:https://dev.mysql.com/doc/connector-python/en/
Psycopg for PostgreSQL :http://initd.org/psycopg/docs/install.html
pymssql for Microsoft MS SQL :http://pymssql.org/en/stable/
then you would connect to the database using specific host, port and authentication credentials but then you could execute the same exact SQL statements.
>>>import pandas as pd
>>>iris_data = pd.read_sql_query("SELECT * FROM Iris", conn)
(reading the O/P into a pandas dataframe)
pandas.read_sql_query takes a SQL query and a connection object
and imports the data into a DataFrame, also keeping the same data types of the database columns
>>>iris_data.head()
>>>iris_data.dtypes
(gives datatypes of columns)
However, sqlite3 is extremely useful for downselecting data before importing them in pandas.
For example you might have 1 TB of data in a table stored in a database on a server machine.
You are interested in working on a subset of the data based on some criterion, 
unfortunately it would be impossible to first load data into pandas and then filter them,
therefore we should tell the database to perform the filtering and just load into pandas the downsized dataset.
>>>iris_setosa_data = pd.read_sql_query("SELECT * FROM Iris WHERE Species == 'Iris-setosa'", conn)
we are selecting iris_setosa rather than rather than all three species.
>>>iris_setosa_data
>>>print(iris_setosa_data.shape)
>>>print(iris_data.shape)
------------------------------------------------------------------------------
NLP:NLTK:
-------------------------------------------------------------------------------
Natural Language Processing, or shortly NLP,is a data science term used to refer
to the interaction of computers,and natural language humans use.
NLP techniques are applied in speech recognition engines,
like Siri, Google Now, or Alexa.
These engines are designed to learn what and how a
human talks over time,and constantly improve their accuracy.
Similarly, automatic translators,like Google Translate or Facebook
automatic translation of statuses use NLP, using some recent very effective
neural network based techniques that take not only words and phrases into account,
but also the context, by looking at the word surrounding
the text they are translating.

NLTK is the most popular Python package for NLP.
It is an open source library that provides modules
for importing, cleaning, pre-processing text data,
in human language,
and then apply computational linguistics algorithms,
or machine learning algorithms, like sentiment analysis,
to these datasets.
It also provides over 50 datasets to start working with,
including the movie database we will be using
in our example notebook.
Note:We can use NLTK download function to download these datasets.

Code:

It also includes many easy-to-use datasets in the `nltk.corpus` package, 
we can download for example the `movie_reviews` package using the `nltk.download` function:
>>>import nltk
>>>nltk.download("movie_reviews")
o/p:true (i.e dataset is downloaded)
You can also list and download other datasets interactively just typing:
nltk.download() in the Jupyter Notebook.
***Once the data have been downloaded, we can import them from `nltk.corpus`.
>>>from nltk.corpus import movie_reviews
(importing the movie reviews we downloaded)
The FILE ID's `fileids` method provided by all the datasets in `nltk.corpus` 
gives access to a list of all the files available.
In particular in the movie_reviews dataset we have 2000 text files,
each of them is a review of a movie, and they are already split in a 
`neg` folder for the negative reviews and a `pos` folder for the positive reviews:
>>>len(movie_reviews.fileids())
o/p:2000
NL techniques depends on large amounts of text
or other linguistics data.
These digital collections are called corpora all together.
Another word you will hear is a corpus,
which is the singular form of corpora.
Therefore:corpus(plural corpora) is a collection of text in 
digital form ,assembled for text processing.
(NLTK provides a download interface to pre-processed text datasets)
After importing nltk, we were able to interact
with a download interface for all of these datasets
using nltk download.
The movie reviews corpus,
was downloaded in your home folder.
Now,as we have imported this dataset
and the dataset has 2000 records(1000 +ve reviews and 1000 0ve reviews)
>>>movie_reviews.fileids()[:5]
(checking the first 5 files in the corpus)
o/p:
['neg/cv000_29416.txt',
 'neg/cv001_19502.txt',
 'neg/cv002_17424.txt',
 'neg/cv003_12683.txt',
 'neg/cv004_12641.txt']
>>>movie_reviews.fileids()[-5:]
(checking the last 5 fils)
O/P:
['pos/cv995_21821.txt',
 'pos/cv996_11592.txt',
 'pos/cv997_5046.txt',
 'pos/cv998_14111.txt',
 'pos/cv999_13106.txt']
 ==> we can see that the files are placed in folders/folders 'neg' and 'pos'.

`fileids` can also filter the available files based on their category, 
which is the name of the subfolders they are located in. 
Therefore we can have lists of positive and negative reviews separately.
>>>negative_fileids = movie_reviews.fileids('neg')
>>>positive_fileids = movie_reviews.fileids('pos')
 (as we can see the o/p's are Lists(due to sq. braces when we displayes top 5 and last 5 files))
 ==> we can use 'len' fn
 i.e >>>len(negative_fileids), len(positive_fileids)
 o/p:1000,1000
 Lets review a file of this data set
 ==> using 'raw' method to inspect a single file
 >>>print(movie_reviews.raw(fileids=positive_fileids[0]))
 Output is a lot of text data,which is the indetail review of the movie.
 What do we do of this text ???
 ==>The first step in Natural Language processing is generally 
 to split the text into words, this process might appear simple
 but it is very tedious to handle all corner cases, see 
 for example all the issues with punctuation we have to 
 solve if we just start with a split on whitespace.
 ==> tokenizing
 
 


Q)What is the correct way to show the last 2 files using movie_reviews.fileids() where movie_reviews is a downloaded dataset?
A)movie_reviews.fileid()[-2:]
B)movie_reviews.fileids([-2:])
C)movie_reviews.fileids()[:-2]
D)movie_reviews.fileids([:-2])
Ans: (A) 


