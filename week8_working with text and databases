The primary data structure for a relational model is a table,
Notice that a table structure in a DB is very similar to a Python DataFrame.
==>DataFrames can be used to load relational tables(DB tables)
   which are also called relations.
This DB table actually represents a set of tuples(rows).
We were informally calling this a record before
but now we call it a tuple.
Therefore:Table is called as relation.
          Row/record is called as Tuple.
Remember the definition of sets,
it's a collection of distinct elements
of the same type.(remember the word 'distinct' and 'Same type')          
In general many systems/DB's will allow duplicate tuples in their relations(tables)
but mechanisms are provided to prevent duplicate entries.

Note:It is important to understand that join
is one of the most expensive,
that is time and space consuming, operations.
As data becomes larger
and the tables contains hundreds of millions of tuples,
the join operation can easily become a bottleneck
in a larger analytical application.
So, for data science
that involves big data,when we need joins,
it's very important to choose
a suitable data management platform
that makes its operation efficient.

Access a DB with Python:
------------------------
We will use the Iris Database
from Kaggle in SQLite format.
It features 50 iris flowers
and their classification into three species.
https://www.kaggle.com/uciml/iris/
The dataset is called/named database.sqlite,
First let's check that the sqlite database is available and display an error message if the file is not available
(assert checks if the expression is True, otherwise throws AssertionError with the error message string provided):
i.e check if the file is present at our location
>>>import os
>>>data_iris_folder_content = os.listdir("A:/Naveed/DS/micromasters/Week8")
>>>error_message = "Error: sqlite file not available"
>>>assert "database.sqlite" in data_iris_folder_content, error_message
Please note that this error message operation
shows an assertion or a good way of capturing errors
in a program using an assert statement.
Assert statements generally help you locate
or identify bugs in your Python programs,
as a way of creating such built-in tests.
If you do a lot of testing,
you'll actually do a lot of assert statements.
We use the sqlite3 package from the Python standard library to connect to the sqlite database:
SQLite comes with your standard Python,
so it's not a library that you need to go and install separately.
>>>import sqlite3
>>>conn = sqlite3.connect('A:/Naveed/DS/micromasters/Week8/database.sqlite')
Now we have a connection object,
and we'll use this connection object
to get a cursor object.That cursor object
is actually our interface to the database.
A sqlite3.Cursor object is our interface to the database, 
llr to execute method that allows to run any SQL query on our database.
>>>cursor = conn.cursor()
>>>type(cursor)
O/P:sqlite3.Cursor
First of all we can get a list of all the tables saved into the database, 
this is done by reading the column name from the sqlite_master metadata table with:
SELECT name FROM sqlite_master
The output of the execute method is an iterator that can be used
in a for loop to print the value of each row.
>>>for row in cursor.execute("SELECT name FROM sqlite_master"):
>>>    print(row)
O/P:('Iris',)
we only have one table==>so we got that one row.
A shortcut to directly execute the query and gather the results is the `fetchall` method:
==>we can use FETCHALL method instead of loop
>>>cursor.execute("SELECT name FROM sqlite_master").fetchall()
>>>sample_data = cursor.execute("SELECT * FROM Iris LIMIT 20").fetchall()
>>>print(type(sample_data))
==> its of type LIST ==> easy to work,but convert it to DataFrame==> more easy to work
>>>sample_data
>>>[row[0] for row in cursor.description]
(checking the colum names of the list)
Note: This way of finding the available tables in a database is specific to sqlite, 
other databases like MySQL or PostgreSQL have different syntax.
Then we can execute standard SQL query on the database, 
SQL is a language designed to interact with data stored in a relational database. 
It has a standard specification, therefore the commands below work on any database.

If you need to connect to another database, you would use another package instead of sqlite3,
Eg:
MySQL Connector for MySQL:https://dev.mysql.com/doc/connector-python/en/
Psycopg for PostgreSQL :http://initd.org/psycopg/docs/install.html
pymssql for Microsoft MS SQL :http://pymssql.org/en/stable/
then you would connect to the database using specific host, port and authentication credentials but then you could execute the same exact SQL statements.
>>>import pandas as pd
>>>iris_data = pd.read_sql_query("SELECT * FROM Iris", conn)
(reading the O/P into a pandas dataframe)
pandas.read_sql_query takes a SQL query and a connection object
and imports the data into a DataFrame, also keeping the same data types of the database columns
>>>iris_data.head()
>>>iris_data.dtypes
(gives datatypes of columns)
However, sqlite3 is extremely useful for downselecting data before importing them in pandas.
For example you might have 1 TB of data in a table stored in a database on a server machine.
You are interested in working on a subset of the data based on some criterion, 
unfortunately it would be impossible to first load data into pandas and then filter them,
therefore we should tell the database to perform the filtering and just load into pandas the downsized dataset.
>>>iris_setosa_data = pd.read_sql_query("SELECT * FROM Iris WHERE Species == 'Iris-setosa'", conn)
we are selecting iris_setosa rather than rather than all three species.
>>>iris_setosa_data
>>>print(iris_setosa_data.shape)
>>>print(iris_data.shape)
------------------------------------------------------------------------------
NLP:NLTK:
-------------------------------------------------------------------------------
Natural Language Processing, or shortly NLP,is a data science term used to refer
to the interaction of computers,and natural language humans use.
NLP techniques are applied in speech recognition engines,
like Siri, Google Now, or Alexa.
These engines are designed to learn what and how a
human talks over time,and constantly improve their accuracy.
Similarly, automatic translators,like Google Translate or Facebook
automatic translation of statuses use NLP, using some recent very effective
neural network based techniques that take not only words and phrases into account,
but also the context, by looking at the word surrounding
the text they are translating.

NLTK is the most popular Python package for NLP.
It is an open source library that provides modules
for importing, cleaning, pre-processing text data,
in human language,
and then apply computational linguistics algorithms,
or machine learning algorithms, like sentiment analysis,
to these datasets.
It also provides over 50 datasets to start working with,
including the movie database we will be using
in our example notebook.
Note:We can use NLTK download function to download these datasets.

Code:

It also includes many easy-to-use datasets in the `nltk.corpus` package, 
we can download for example the `movie_reviews` package using the `nltk.download` function:
>>>import nltk
>>>nltk.download("movie_reviews")
o/p:true (i.e dataset is downloaded)
You can also list and download other datasets interactively just typing:
nltk.download() in the Jupyter Notebook.
***Once the data have been downloaded, we can import them from `nltk.corpus`.
>>>from nltk.corpus import movie_reviews
(importing the movie reviews we downloaded)
The FILE ID's `fileids` method provided by all the datasets in `nltk.corpus` 
gives access to a list of all the files available.
In particular in the movie_reviews dataset we have 2000 text files,
each of them is a review of a movie, and they are already split in a 
`neg` folder for the negative reviews and a `pos` folder for the positive reviews:
>>>len(movie_reviews.fileids())
o/p:2000
NL techniques depends on large amounts of text
or other linguistics data.
These digital collections are called corpora all together.
Another word you will hear is a corpus,
which is the singular form of corpora.
Therefore:corpus(plural corpora) is a collection of text in 
digital form ,assembled for text processing.
(NLTK provides a download interface to pre-processed text datasets)
After importing nltk, we were able to interact
with a download interface for all of these datasets
using nltk download.
The movie reviews corpus,
was downloaded in your home folder.
Now,as we have imported this dataset
and the dataset has 2000 records(1000 +ve reviews and 1000 0ve reviews)
>>>movie_reviews.fileids()[:5]
(checking the first 5 files in the corpus)
o/p:
['neg/cv000_29416.txt',
 'neg/cv001_19502.txt',
 'neg/cv002_17424.txt',
 'neg/cv003_12683.txt',
 'neg/cv004_12641.txt']
>>>movie_reviews.fileids()[-5:]
(checking the last 5 fils)
O/P:
['pos/cv995_21821.txt',
 'pos/cv996_11592.txt',
 'pos/cv997_5046.txt',
 'pos/cv998_14111.txt',
 'pos/cv999_13106.txt']
 ==> we can see that the files are placed in folders/folders 'neg' and 'pos'.

`fileids` can also filter the available files based on their category, 
which is the name of the subfolders they are located in. 
Therefore we can have lists of positive and negative reviews separately.
>>>negative_fileids = movie_reviews.fileids('neg')
>>>positive_fileids = movie_reviews.fileids('pos')
 (as we can see the o/p's are Lists(due to sq. braces when we displayes top 5 and last 5 files))
 ==> we can use 'len' fn
 i.e >>>len(negative_fileids), len(positive_fileids)
 o/p:1000,1000
 Lets review a file of this data set
 ==> using 'raw' method to inspect a single file
 >>>print(movie_reviews.raw(fileids=positive_fileids[0]))
 Output is a lot of text data,which is the indetail review of the movie.
 What do we do of this text ???
 ==>The first step in Natural Language processing is generally 
 to split the text into words, this process might appear simple
 but it is very tedious to handle all corner cases,
What are corner cases?
They include inconsistent use of punctuation,
or contractions, or shortened versions of words.
They can also be hyphenated words
that include characters like the
'New York-based' example here.
How do we Tokenize such cases?
Nltk offers libraries to remedy these challenges.
we will first use a simple white space based Tokenizer.
Then, we will learn how to do it better and easier
using nltk.

==> tokenizing:
simple white space based Tokenizer.
==> use string-splitter function in Python for this
Say we below text:
>>>romeo_text = """Why then, O brawling love! O loving hate!
>>>   O any thing, of nothing first create!
>>>   O heavy lightness, serious vanity,
>>>   Misshapen chaos of well-seeming forms,
>>>   Feather of lead, bright smoke, cold fire, sick health,
>>>   Still-waking sleep, that is not what it is!
>>>   This love feel I, that feel no love in this."""
>>>romeo_text.split()
==> we see that punctuations like exclamations,periods,etc
are still with the words
==>`nltk` has a sophisticated word tokenizer trained on English
named `punkt`, to remove these punctuations
we first have to download its parameters: 
>>>nltk.download("punkt")
Then we can use the `word_tokenize` function to properly tokenize
this text, compare to the whitespace splitting we used above.
>>>romeo_words = nltk.word_tokenize(romeo_text)
>>>romeo_words
==> we can see that the words are tokenized properly now.
i.e say we have 'then.'
==> we have 2 tokes 1st is 'then' and 2nd is '.' using punkt
The good news is all corpora in nltk
already provides a way to generate Tokenized words
for each data file.
==>movie_reviews corpus already has direct access to tokenized text with the words method
>>>movie_reviews.words(fileids=positive_fileids[0])

==>creating Bag of words model:
Bag of words: text as unordered collection of words.
Bag-of-words model is a very simple representation
of a body of text as a loose set of words.
It flattens any text into an unordered collection of words.
Although it disregards the sentence structure
associated to the words, this simple technique
is pretty useful to identify a topic or sentiment in text,
like if a product review has
a negative or positive sentiment,
or what a body of text talks about.
We can use the words in a feature matrix
where each word is a column, and each text body
or review in our movie example,
is a row that has boolean data values.
A cell in the review row gets assigned true
if the word appears in the review,
and false if it doesn't.
From the bag-of-words model
we can build features to be used by a classifier
and here we assume that each word is a feature
that can either be true or false.
We implement this in Python as a dictionary,
where each word in a sentence we associate with true,
and if a word is missing, that would be the same
as assigning false

before further analysis it is often practice to filter our stopwords
and maybe even the punctuations from the bag-of-words.
Stopwords are words like the, that, and is,
which occur a lot but don't have a big significance
in identifying the context of the text being processed.
>>>{word:True for word in romeo_words}
(assigning True for each word/token in romeo words)
So we'll have, as you see here, a dictionary
where each word in romeo_words are assigned true,
and there are no false values
because we are not assigning false for anything.
>>>type(_)
o/p: dict
You would remember taht underscore(_) here is the last output
that goes into the standard out.
So if you had assigned that dictionary to a variable,
you need to write the name
of that variable within this type.
(i.e we can also do >>>dict1={word:True for word in romeo_words}
                    >>>type(dict1)
)
Lets generalize it by moving this code into a python function.
>>>def build_bag_of_words_features(words):
>>>    return {word:True for word in words}
==> fn name = build_bag_of_words_features
   this will accept a set of words and return a dictionary for it.
So let's run the function and see if we get that same output
>>>build_bag_of_words_features(romeo_words)
#removing the stopwords by downloading the stopword nltk corpus for english stopwords
>>>nltk.download("stopwords")
importing string class
>>>import string
these are the punctuations present in string class
>>>string.punctuation
O/P:'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
==> we will now remove the stopwords and the punctuations present above.
==> capturing the useless words
>>>useless_words = nltk.corpus.stopwords.words("english") + list(string.punctuation)
>>>#useless_words
>>>#type(useless_words)
Now we will actually update our bag-of-words function
that we built with an if statement
to check if the word exists in useless_words
and skip that word if it does.
>>>def build_bag_of_words_features_filtered(words):
>>>    return {
>>>        word:1 for word in words \
>>>        if not word in useless_words}
==>using the sam sub fn 'word' as before
   replaced True with '1' (since True is 1 and False is 0)
   i.e for each true word,if the word is not present in useless words==> add it to the dictionary)
>>>build_bag_of_words_features_filtered(romeo_words)   
   
==> Plotting Frequency of Words:
Generally we use plotting to compare 2 datasets.
Using the `.words()` function with no argument we 
can extract the words from the entire dataset.
>>>all_words = movie_reviews.words()
after extracting the words...lets count them.
>>>len(all_words)/1e6
O/P:1.58382
==> we have around 1.6 million words.
Now lets count the words after filtering stopwords.
>>>filtered_words = [word for word in movie_reviews.words() if not word in useless_words]
#>>>type(filtered_words)
>>>len(filtered_words)/1e6
O/P:0.710579 i.e .7 million
The `collection` package of the standard library contains 
a `Counter` class that is handy for counting frequencies 
of words in our list.
==>Let's create a counter object using these filtered words.
we'll start by importing counter from collections.
And we'll give counter this filtered words.
And turn that into a counter object.
>>>from collections import Counter
>>>word_counter = Counter(filtered_words)
Once we have this word_counter object successfully ran,
We can use any function the counter class provides us,
or the properties of this class.
==>we have a method called `most_common()` from the 
Counter class to get the words with the higher count.
>>>most_common_words = word_counter.most_common()[:10]
>>>most_common_words
Now we would like to have a visualization of this using matplotlib.
First we want to use the Jupyter magic function
%matplotlib inline
To setup the Notebook to show the plot embedded 
into the Jupyter Notebook page, you can also test:
%matplotlib notebook
for a more interactive plotting interface which 
however is not as well supported on all platforms and browsers.
>>>%matplotlib notebook
#>>>%matplotlib inline
>>>import matplotlib.pyplot as plt
We need to sort this list,
before we can plot it actually.

We can sort the word counts and plot their values
on Logarithmic axes to check the shape of the distribution. 
This visualization is particularly useful if comparing 2 
or more datasets, a flatter distribution indicates a large
vocabulary while a peaked distribution a restricted vocabulary
often due to a focused topic or specialized language.

==>we had sorted word_counter and the values in it.
And we assigned that to a list called sorted_word_counts.
We'll give those sorted word counts to 
the loglog fn to create that logarithmic plot.
>>>sorted_word_counts = sorted(list(word_counter.values()), reverse=True)
>>>plt.loglog(sorted_word_counts)
>>>plt.ylabel("Freq")
>>>plt.xlabel("Word Rank");
We can also plot the histogram of sorted word counts,
which displays how many words have a count,
in a specific range.
Not per word but the words are bent
to be together, if they have a similar count.
Since we have many words with low frequencies,
in our corpus.
>>>plt.hist(sorted_word_counts, bins=50);
we can see that the distribution is highly peaked at low counts
(since we have many words whose frequency is less), i.e. most of the words
appear with a low count, so we better display it on 
semilogarithmic axes to inspect the tail of the distribution.
>>>plt.hist(sorted_word_counts, bins=50, log=True);

Sentiment Analysis:
---------------------------
We now use the bag-of-words model we created 
in a classifier for Sentiment Analysis of movie reviews.
what is Sentiment Analysis?
The term refers to the activity of identifying attitude
or emotion encoded in a body of text,
like a product review(+ve or -ve) or work of literature.
Classification using machine learning
is a technique used for sentiment models.
As you would remember, classification is a
supervised activity and requires labels
from a ground truth data.
This is where we will take advantage of
bag-of-words and a curated negative
and positive reviews we downloaded.
We will use our previously implemented bag-of-words function
to create a positive or negative label
for each review bag-of-words.
We will use Naive Bayes Classifier for this task.

Naive Bayesclasifier:
is a very simple classifier with
a probabilistic approach to classification.
What this means is that the relationships between
the input features and the class labels
is expressed as probabilities.
So, given the input features, for example,
the probability for each class is estimated.
The class with the highest probability
then determines the label for the sample.
==>Naive bayes is a simple classifier
based on conditional probability.
-->In training phase,it detects the probability that
each feature(word) appears in a category(+ve or -ve).
-->Once Trained,it collects the 'votes' for all words
in the new review and finds the most probable label.

**We will use Naive Bayes from nltk this time, not scikit-learn.
==>Training a Classifier for Sentiment Analysis:
Using our `build_bag_of_words_features` fn we can 
build separately the negative and positive features.
Basically for each of the 1000 negative and for the 
1000 positive review, we create one dictionary of the
words and we associate the label "neg" and "pos" to it.
(We are lucky that the database is curated
to separate positive and negative reviews.
So we will use this as ground truth data
and build two dictionaries as a bag-of-words
for positive and negative reviews.)
>>>negative_features = [
>>>    (build_bag_of_words_features_filtered(movie_reviews.words(fileids=[f])), 'neg') \
>>>    for f in negative_fileids
>>>]
we can print a samoke record and test
>>>print(negative_features[3])
lly for positive features
>>>positive_features = [
>>>    (build_bag_of_words_features_filtered(movie_reviews.words(fileids=[f])), 'pos') \
>>>    for f in positive_fileids
>>>]
>>>print(positive_features[6])
Now we have our 2 features and remember that
each of these feature has 1000 labels/records in it.
(1000 records each for +ve lable and -ve labels respectively)
>>>from nltk.classify import NaiveBayesClassifier
we are doing 80-20 split
When we provide the first 800 rows
in each feature, it's 80%, right, 1,000 times 80%.
So we'll store that number, 800,
in a variable called split.
And we'll use that split to slice the first 800 for training
and the remaining 200 for testing later on.
>>>split = 800
==>Now We are using the Naive Bayes Classifier
to train it, with the first 800 positive features
and the first 800 negative features.
>>>sentiment_classifier = NaiveBayesClassifier.train(positive_features[:split]+negative_features[:split])
(Remember they had labels pos and neg.)
we'll call it Sentiment Classifier.
We'll now check the accuracy of the model we built.
using the classification's accuracy fn,
it's the utility of the classification.
Lets check after training what is the accuracy on the training set,
i.e. the same data used for training, we expect this to be a very 
high number because the algorithm already "saw" those data. 
Accuracy is the fraction of the data that is classified correctly,
we can turn it into percent.
>>>nltk.classify.util.accuracy(sentiment_classifier, positive_features[:split]+negative_features[:split])*100
O/p: 98.065
==> preety good as expected
Now lets check how the model behave on the remaining 20 %
>>>nltk.classify.util.accuracy(sentiment_classifier, positive_features[split:]+negative_features[split:])*100
O/p:71.75
===>Accuracy here is around 70% which is pretty good for
such a simple model(we can further improve our model)if we 
consider that the estimated accuracy for a person is about 80%.
There if there were a human instead of 
machine he would have predicted 80% correctly.
We can finally print the most informative features,
i.e. the words that mostly identify a +ve or -ve review.

Remember, we had a large vocabulary and
the Sentiment Classifier used all the words,
but which of those words gave us this highish accuracy?
==> we have a fn for that as well to tell which 
words were most informative in the dataset.
>>>sentiment_classifier.show_most_informative_features()
O/p:
Most Informative Features
       outstanding = 1                 pos : neg    =     13.9 : 1.0
         insulting = 1                 neg : pos    =     13.7 : 1.0
        vulnerable = 1                 pos : neg    =     13.0 : 1.0
         ludicrous = 1                 neg : pos    =     12.6 : 1.0
       uninvolving = 1                 neg : pos    =     12.3 : 1.0
            avoids = 1                 pos : neg    =     11.7 : 1.0
        astounding = 1                 pos : neg    =     11.7 : 1.0
       fascination = 1                 pos : neg    =     11.0 : 1.0
         affecting = 1                 pos : neg    =     10.3 : 1.0
            darker = 1                 pos : neg    =     10.3 : 1.0
==> words like outstanding,insulting etc had more effect on our output

Other uses of NLP:
1)Estimating Oscar Winner/;
A company (I do not remember which) claims to 
have predicted the winner at the Oscars 2016 
(The Revenant) using only reviews posted by the
online community on popular movie review websites. 
is a Prime example of NLP.

2)Using sentiment evaluation to target individuals
that may have mental health issues.:
Sentiment evaluation of social media could be used
to target relevant advertising/websites to people 
with mental health issues, e.g. people that repeatedly 
post sad statuses could be targeted with a helpline advert.
This principle is already being applied with targeted 
marketing based on keyword searches, so why not use it
for a good cause as well.

3)Predict a natural disaster for insurers company.

Q&A:
-----
Q)What is the correct way to show the last 2 files using movie_reviews.fileids() where movie_reviews is a downloaded dataset?
A)movie_reviews.fileid()[-2:]
B)movie_reviews.fileids([-2:])
C)movie_reviews.fileids()[:-2]
D)movie_reviews.fileids([:-2])
Ans: (A) 
Q)True or False: We can use ‚Äò1‚Äô and ‚ÄòTrue‚Äô interchangeably.
a)True
b)False
Ans: A (get it confirmed from someone once)
Q)What is the benefit of a log graph over a graph that has not modified the scales?
a)Log scales allow a large range of values that are compressed into one point of the graph to be shown. 83%
b)Log scales are significantly better in representing every type of data compared to unmodified scale graphs. 14%
c)There is no outright advantage to converting a graph into a log scale. 3%
Ans: A (get it confirmed from someone once)


------------------------------------------------------------------------------------
Accessing Twitter API:
---------------------------
Twitter offers a Web Application Programming
Interface,or shortly API,
to access all Tweets on their website.
Through this API, we can send queries to Twitter
to search for data related to various topics,
find out about trends, user interactions,
and many other analysis tasks spelled on Twitter data.

This will require authentication on Twitter.
So please create a Twitter account if you don't have one
and this account won't be sufficient.
We will also need to create an app that identifies
all the requests we send to the platform.


==> go to https://apps.twitter.com/ and create a new application
==> give a project name 
==> give URL as http://google.com
==> application is created
Now create access token in keys a nd access tokens tab
while consumer key and consumer key access token are created by default.

->Twitter implements OAuth 1.0A as its standard authentication mechanism,
and in order to use it to make requests to Twitter's API, 
you'll need to go to https://dev.twitter.com/apps and create a sample application.
or (https://apps.twitter.com/)
->Choose a name for your application, write a description and use http://google.com for the website.
->Under Key and Access Tokens, there are four primary identifiers you'll need to note for an OAuth 1.0A workflow:
consumer key,
consumer secret,
access token, and
access token secret (Click on Create Access Token to create those).
->Note that you will need an ordinary Twitter account in order to login,
create an app, and get these credentials.
->The first time you execute the notebook, 
add all credentials so that you can save them in the pkl file, 
then you can remove the secret keys from the 
notebook because they will just be loaded from the pkl file.
->The pkl file contains sensitive information that 
can be used to take control of your twitter acccount, do not share it.

Code:
>>>import pickle
>>>import os
>>>if not os.path.exists('secret_twitter_credentials.pkl'):
>>>    Twitter={}
>>>    Twitter['Consumer Key'] = ''
>>>    Twitter['Consumer Secret'] = ''
>>>    Twitter['Access Token'] = ''
>>>    Twitter['Access Token Secret'] = ''
>>>    with open('secret_twitter_credentials.pkl','wb') as f:
>>>        pickle.dump(Twitter, f)
>>>else:
>>>    Twitter=pickle.load(open('secret_twitter_credentials.pkl','rb'))

==> first we are creating a obect called twitter
and storing the creds in that.If the pickled 
credentials exist,it will just load the credentials
from the pickle file into the Twitter object.

Pickle:
->Pickle is a Python utility module
to save any Python object or data structure on disk.
Pickle will do something special called serialization.
We need it to convert any Python object(in this case,
this Twitter object) into a character stream
so the object can be created later in Python
Reconstruction of that object is called deserialization.

==>Now Lets Install the `twitter` package to interface with the Twitter API.
>>>!pip install twitter

Now we'll create a new object called authentication, or auth.
using the Twitter object and Twitter API
>>>import twitter
>>>auth = twitter.oauth.OAuth(Twitter['Access Token'],
>>>                           Twitter['Access Token Secret'],
>>>                           Twitter['Consumer Key'],
>>>                           Twitter['Consumer Secret'])
Now we use this authentication
and create a Twitter API object called twitter_api.
This object is what we will use to start
accessing the data via the Twitter API.
>>>twitter_api = twitter.Twitter(auth=auth)
>>># Nothing to see by displaying twitter_api except that it's now a
>>># defined variable
>>>print(twitter_api)
O/p:<twitter.api.Twitter object at 0x000001A03574DA58>
(==> type of the objject is twitter)

Let's now start exploring the trends in the Twitter data.
When we talk about Twitter trends we mean popular hashtags.
Twitter categorizes them by location
for places like the world or United States
or locally for San Diego.
The world trends for example show up on the left
when you go to Twitter.com.
Twitter tracks trends using the 'Yahoo where On 
Earth ID' also called (as  Yahoo WOEID) for each major location.
i.e Twitter identifies a location with a integer 
named WOEID,this value for world is '1'.
Eg: >>>world_trends = twitter_api.trends.place(_id=1)

Using the 'trends.place' for the Twitter API object
we can get the top 50 trends for any location.
You can look it up online for your location
using the link here:
http://woeid.rosselliot.co.nz/
==> back to code:
>>>WORLD_WOE_ID = 1
>>>IND_WOE_ID = 23424848
>>>LOCAL_WOE_ID=90883135
>>># Prefix ID with the underscore for query string parameterization.
>>># Without the underscore, the twitter package appends the ID value
>>># to the URL itself as a special case keyword argument.
>>>world_trends = twitter_api.trends.place(_id=WORLD_WOE_ID)
>>>us_trends = twitter_api.trends.place(_id=IND_WOE_ID)
>>>local_trends = twitter_api.trends.place(_id=LOCAL_WOE_ID)
what we receive back is a trend object
with a response that's in JSON.
>>>world_trends[:2]
We can see the response for example for world trends
by querying the first two records.

JSON:
JSON is a file format which 
internet applications use to communicate
semi-structured information.
It is similar to XML but it's more 
concise.It is roughly equivalent
to nested Python dictionaries and lists.

WOEID:
A WOEID (Where On Earth IDentifier) is a unique 
32-bit reference identifier, 
originally defined by GeoPlanet and now assigned 
by Yahoo!, that identifies any feature on Earth.
WOEIDs are used by a number of other projects, including 
Flickr OpenStreetMap, Twitter, and zWeather.
BOSS PlaceFinder API:
BOSS PlaceFinder is a premium geocoding Web service that 
helps developers make their applications location-aware 
by converting street addresses or place names into 
geographic coordinates and vice versa.
(Access to the BOSS APIs will continue until 
March 31, 2016. Moving forward, customers leveraging 
the BOSS JSON Search API can instead use YPA(Yahoo partner ads), 
a Javascript Solution that provides algorithmic web results with 
search ads for publishers who manage their own search engine results pages (SERPs)).

==>back to code:
>>>trends=local_trends
>>>print(type(trends))
>>>print(list(trends[0].keys()))
#gives the keys(generally i.e ['trends', 'as_of', 'created_at', 'locations']
>>>print(trends[0]['trends'])
#printing all the tresds and their info.
Let's import that JSON module 
and use it in something useful.
So in this line we are using the dumps function of JSON
to create a better version of the same output.
>>>import json
>>>print((json.dumps(us_trends[:2], indent=1)))
(#indent = 1 ==> indent the next level/parenthesis) with 1 space)
Let's create sets(set objects) of these trends
for each location.
And then we'll find the commonalities between trends
for those locations.
So in other words
we'll find the intersections of these sets.
==> lets the name for all trends
using a loop set trend name and for trend and world trends
for all the locations
>>>trends_set = {}
>>>trends_set['world'] = set([trend['name'] 
>>>                        for trend in world_trends[0]['trends']])
>>>trends_set['us'] = set([trend['name'] 
>>>                     for trend in us_trends[0]['trends']]) 
>>>trends_set['san diego'] = set([trend['name'] 
>>>                     for trend in local_trends[0]['trends']]) 
We have now this trends set object
and there are three sets in it, one for the world,
one for India, one for San Diego.
Now lets create a for loop
that joins all the trends for a particular location
and prints them in pretty/better format.
We'll first join the trends for world, then India,
then San Diego.
>>>for loc in ['world','India','san diego']:
>>>    print(('-'*10,loc))
>>>    print((','.join(trends_set[loc])))
now how do we create intersections
of these texts, sets?
We will make use of Python set objects,
which we have already created.
These set objects will give us an intersection 
function.So let's use that.
>>>print(trends_set)
the first one we'll give us the intersection
of world and India, the second one will give us
the intersection of San Diego with what's going on in India.
>>>print(( '='*10,'intersection of world and India'))
>>>print((trends_set['world'].intersection(trends_set['India'])))
>>>print(('='*10,'intersection of us and san-diego'))
>>>print((trends_set['san diego'].intersection(trends_set['India'])))
How about if we want to search Twitter for 
tweets related to a particular topic or hashtag?
We will see that each tweet is not only 140 character string
but also comes with many other information related to it.
Those are they keys of the dictionary of each tweet.
We will use these metadata
to extract interesting information
about tweets that we receive
in response to our search query.
>>>topic = '#MTVAwards' 
>>>number = 100
>>># See https://dev.twitter.com/docs/api/1.1/get/search/tweets
>>>search_results = twitter_api.search.tweets(q=topic, count=number)
>>>statuses = search_results['statuses']
>>>#len(statuses)
>>>print(statuses)
>>>#print(statuses[0]) ==> gives the first tweet all info
==> gives the latest 100 tweets.
we get a bunch of records for each tweet in JSON format.
Link whn it was created 
and it's a retweet and things like that.
There are lots of metadata about a particular tweet
including the text for that tweet.
Twitter often returns duplicate results,
we can filter them out checking for duplicate texts.
(as there might be same tweets,retweets etc)
>>>all_text = []
>>>filtered_statuses = []
>>>for s in statuses:
>>>    if not s["text"] in all_text:
>>>        filtered_statuses.append(s)
>>>        all_text.append(s["text"])
>>>statuses = filtered_statuses     
('text' contains the tweet message)
i.e for each status(S) in statuses,if tweet 
text is not present in all texts==>then append.
>>>len(statuses)
We can also display the statuses, all this text,
using slicing out the text from the statuses object,
the search results.
>>>[s['text'] for s in search_results['statuses']]
since this was in JSON,
we can use the JSON dumps again.
>>># Show one sample search result by slicing the list...
>>>print(json.dumps(statuses[0], indent=1))
we can also analyse individual statuses
>>># The result of the list comprehension is a list with only one element that
>>># can be accessed by its index and set to the variable t
>>>t = statuses[0]
>>>#[ status for status in statuses 
>>>#          if status['id'] == 316948241264549888 ][0]
>>># Explore the variable t to get familiarized with the data structure...
>>>print(t['retweet_count'])
>>>print(t['retweeted'])
o/p:71
   false
Extracting text,hashtags,screen names etc
>>>status_texts = [ status['text'] 
>>>                 for status in statuses ]
>>>screen_names = [ user_mention['screen_name'] 
>>>                 for status in statuses
>>>                     for user_mention in status['entities']['user_mentions'] ]
>>>hashtags = [ hashtag['text'] 
>>>             for status in statuses
>>>                 for hashtag in status['entities']['hashtags'] ]
>>># Compute a collection of all words from all tweets
>>>words = [ w 
>>>          for t in status_texts 
>>>              for w in t.split() ]
(this split() fn is from string class)
>>># Explore the first 5 items for each list...
>>>print(json.dumps(status_texts[0:5], indent=1))
>>>print(json.dumps(screen_names[0:5], indent=1)) 
>>>print(json.dumps(hashtags[0:5], indent=1))
>>>print(json.dumps(words[0:5], indent=1))
Next we'll find the frequencies of these words.

Frequescy Analysis:
As you remember from the last lesson,
the collections package in the standard library
contains the counter class,
i.e from collections import counter
which is handy for creating frequency distributions.
It gets lists and counts how many times
each items is repeated in a list.
Its most_common() fn/method
returns the sorted counts.
The final output works
but it's hard to read with all that syntax.
We will create a nicely formatted table out of this output.
Advanced string formatting in python:
>>>print("{:20} | {:>6} ".format(k,v))
{:20} format(s) pad the string to 20 spaces in the end
{:^20} format(s) centered in the middle of 20 spaces
{:>20} format(s) same as centered format but it is right aligned
we can check https://pyformat.info

Creating a basic frequency distribution from the words in tweets.
Now we are going to use again the counter
like we did in our NLP example.
To count the frequencies of all
the words in the Tweets.
>>>from collections import Counter
>>>for item in [words, screen_names, hashtags]:
>>>    c = Counter(item)
>>>    print(c.most_common()[:10]) # top 10
>>>    print()
==> we can see the most common words  from each list,
but they are in dictionary kinda format
The output is still hard to read.
It is a list of tuples so it has a lot
of syntax like parenthesis
and the rectangular parenthesis,
square parenthesis, codes, and things like that.
There's actually a better way to do it in Python,
using advanced string formatting.
And we'll use that now to display
the same output in a better format.
==>Creating a prettyprint function to display 
tuples in a nice tabular format
>>>def prettyprint_counts(label, list_of_tuples):
>>>    print("\n{:^20} | {:^6}".format(label, "Count"))
>>>    print("*"*40)
>>>    for k,v in list_of_tuples:
>>>        print("{:20} | {:>6}".format(k,v))
This fn takes, a list of tuples and a label.
And writes out a nicely formatted table
of data objects,or data structures.
In first inner line of fn ,
we create the labels on top.
The second line will give us 40 stars.
Exactly 40, padded nicely.
And for each k and v in this list of tuples,
they are the elements of the list of tuples
we are going to receive.
We will pad the k
to be printed with 20,
completed to 20 characters by spaces behind it.
And let's see,
this is going to left align the word.
==>calling the fn
>>>for label, data in (('Word', words), 
>>>                    ('Screen Name', screen_names), 
>>>                    ('Hashtag', hashtags)):
>>>    
>>>    c = Counter(data)
>>>    prettyprint_counts(label, c.most_common()[:10])
We are loading the data into a counter.
And we are giving this counter
and the labels to our prettyprint function.
o/p: ==> we get count for each word
       Word         | Count 
****************************************
RT                   |     34
#MTVAwards           |     28
the                  |     17
de                   |     12
to                   |     10

    Screen Name      | Count 
****************************************
SGNewsSpain          |      9
MTV                  |      5
EmmaWatson           |      3
GabrielConte         |      2
Realisadiamond       |      2

      Hashtag        | Count 
****************************************
MTVAwards            |     29
SelenaBBMAs          |      9
mtvawards            |      7
ToyotaCHR            |      2
Beyonce              |      1

So next, we would like to find the most popular Tweets
by ordering them by the number of RETweets.
For this, we would like to print their count,
author name and their complete text.
This can be achieved by creating a list of tuples
with those three elements,
making sure the reTweet count is first.
>>>retweets = [
>>>            # Store out a tuple of these three values ...
>>>            (status['retweet_count'], 
>>>             status['retweeted_status']['user']['screen_name'],
>>>             status['text'].replace("\n","\\")) 
>>>            # ... for each status ...
>>>            for status in statuses 
>>>            # ... so long as the status meets this condition.
>>>                if 'retweeted_status' in status
>>>           ]
now we need to create a
prettyprint fn to start printing them.
And finally, we can print them in tabular format.
We can build another `prettyprint` function to 
print entire tweets with their retweet count.
We also want to split the text of the tweet 
in up to 3 lines, for good view in our notebook cell.
(as 140 characters might not fit in code cell block,
moreover, we want to handle the k's of short Tweets
that don't need all of the three lines
with nested statements.)
>>>row_template = "{:^7} | {:^15} | {:50}"
>>>def prettyprint_tweets(list_of_tuples):
>>>    print()
>>>    print(row_template.format("Count", "Screen Name", "Text"))
>>>    print("*"*60)
>>>    for count, screen_name, text in list_of_tuples:
>>>        print(row_template.format(count, screen_name, text[:50]))
>>>        if len(text) > 50:
>>>            print(row_template.format("", "", text[50:100]))
>>>            if len(text) > 100:
>>>                print(row_template.format("", "", text[100:]))
because we are trying to find the most popular reTweets.
We'll sort those ReTweets
and we'll give the first five line as a slice
to prettyprint Tweets.
>>># Slice off the first 5 from the sorted results and display each item in the tuple
>>>prettyprint_tweets(sorted(retweets, reverse=True)[:10])
O/p:
 Count  |   Screen Name   | Text                                              
************************************************************
 12354  |       MTV       | RT @MTV: This is too precious for words. #MTVAward
        |                 | s https://t.co/Xu7BpoZL84                         
 8860   |       MTV       | RT @MTV: Seeing the @Stranger_Things cast with the
        |                 |  @13ReasonsWhy cast at the #MTVAwards is too much 
        |                 | for my heart to handle üíó https://t.co/p‚Ä¶          
 8595   |       MTV       | RT @MTV: Let @dylanobrien guide you through a firs
        |                 | t look at Maze Runner: The Death Cure, exclusively
        |                 |  for the #MTVAwards tonight at 8/7c! üí•‚Ä¶           
 6397   |   EmmaWatson    | RT @EmmaWatson: Thank you @MTV for a wonderful eve
        |                 | ning and thank you to everyone who voted for me! ‚ù§
        |                 | Ô∏èüçø #MTVAwards @beourguest                         
 5427   |  chrissyteigen  | RT @chrissyteigen: Yes, I do my own stunts. New ep
        |                 | isode of @spikelsb tonight before the #mtvawards a
        |                 | t 7:30pm EST! @zendaya vs @TomHolland19‚Ä¶  

FAQ:
Q)What is the reason to prefix id with an underscore for query string parameterization with the twitter object?
a)there is no need in the function, it is there to make it easier on the reader
b)it is needed because it is part of the parameter name
c)it is needed for the twitter object to know the type of output from previous line
d)without it, twitter package appends the value to the URL
Ans: D
Q)Which of the following code snippets is a conditional statement?
A)for s in statuses:
B)if not s[‚Äútext‚Äù] in all_text:
C)[s[‚Äòtext‚Äô] for s in search_results[‚Äòstatuses‚Äô]
D)statuses = filtered_statuses
Ans :B
