Machine learning: is a field of study that focuses on computer systems that can learn from data.
That is, these systems, often called models,can learn to perform a specific task
by analyzing lots of examples for a particular problem.

Eg:identify a cat after analysisng lots of image of cats
The Machine Learning algorithm is programmed to learn from the data that 
there's nothing in the algorithm or program which directly aims to learn the given task.
** i.e Machine Learning models are not given the step by step instructions on how to recognize
the image of a cat.Instead, the model learns on its own what features
are important in determining that a picture contains a cat from the data that it has analyzed.

*Because the model learns to perform this task from data,it's good to note that the amount and quality of data
available for building the model are important factors in how well the model learns from the task.

Therefore,Machine Learning models learn from data to perform a task
without being explicitly programmed.

Clasification:
In classification,the goal is to predict the category of the input data.
The categories to be predicted are called classifications.
Eg:predicting the weather as being sunny,rainy, windy or cloudy.
If the predicted output is b/w 2 values ==> it is called binary Classification.

Regression:
When your model has to predict a numeric value instead of a category,
then the task becomes a regression problem.
Eg:predict the price of a stock.
The stock price is a numeric value,not a category, so this is a regression task.
If you were to predict whether the stock price will rise or fall,then that would 
be a classification problem but if you're predicting the actual price of the stock,
then that's a regression problem.

To summarize,in classification you're predicting a category,
and in regression,you're predicting a numeric value.

Clustering:
In cluster analysis,the goal is to organize similar items in your dataset into groups.
A very common application of cluster analysis is referred to as customer segmentation.
This means that you're separating your customer base into different groups or segments
based on customer types.
For example,
it would be beneficial to segment your customers into seniors, adults and teenagers.

association analysis:
The goal here is to come up with a set of rules
to capture associations between items or events.
The rules are used to determine when items or events occur together.
A common application of association analysis is known as market basket analysis,
which is used to understand customer purchasing behavior.
For example,
->association analysis can reveal that banking customers
who have checking or deposit accounts also tend to be interested 
in other investment vehicles such as money market accounts.
->Other common applications can be recommendation of similar items
based on purchasing or browsing history of customers.
->Identification of web pages that are often accessed together
can also provide us a good basis for association analysis.

For the techniques we've discussed here,there are two ways of conducting 
the learning itself.These categories are referred to as 
A)supervised Learning and B)unsupervised Learning

In supervised approaches,(Target is available)
the target, which is what the model is predicting,is provided.
This is referred to as having labeled data because the target is 
labeled for every sample that you have in your dataset.
Referring back to our example of predicting the weather category
of sunny, windy, rainy or cloudy,every sample in the dataset
is labeled as being one of these four categories.
So, the data is labeled and predicting the weather category is a supervised task.
**In general,classification and regression are supervised approaches.

In unsupervised approaches,(Target is not available)
on the other hand,the target that the model is predicting
is unknown or unavailable.This means that you have unlabeled data,
so you can't train using these labels.Going back to our cluster 
analysis example of segment customers into different groups,the samples
in your data are not labeled with the correct group.
Instead, the segmentation is performed using a clustering technique
to group items based on characteristics of what they have in common.
Thus, the data is unlabeled and the task of grouping customers
into different segments is an unsupervised one.
**In general, cluster analysis and association analysis are unsupervised approaches.

Terminology in ML:
Sample:
It is an instance/example of an entity in your data.It is typically a row in your dataset.

ID    Name     mat     phy 
1     A1        11      20
2     A2        14      28
3     A3        18      22

==> row's with ID's 1,2,3 are called Samples
and coloumns(ID,Name,Mat and Phy) are called Variables/features of sample.
and Each sample has 4 values(variables) associated with it.
We call these different values as variables of the sample
and sometimes refer to them as features of the sample.
A variable captures specific characteristics of each entity.

There are many names for variables in addition to feature.
like: feature, column,dimension, attribute and field etc.

A variable holding Numerical values is called as Numerical/quantitative variable.
A variable holding characters/strings ar called as categorial/nominal/qualitative variable.

Scikit-learn: 
->An open source ML library in Python
->It is built on top of NumPy, SciPy, and matplotlib like many other Python libraries.
->It can be used for end to end Machine Learning in python.
i.e entire data science process including Machine Learning,
data cleaning, and data transformations,.etc can be done by scikit-learn.

Scikit Learn Provides fns for preprocessing such as:
->fns for converting raw feature vectors into suitable formats
-> it providea API's for
  ->scaling of features: remove mean and keep unit variance
  ->Normalization to have unit form
  ->binarization to turn data into 0 or 1 form
  ->One hot encoding for categorial features
  ->handlin of missing values
  ->generating higher order features
  ->built custom transformations
Scikit-learn also provides built-in functions for many
Machine Learning algorithms ready for modeling and analysis.
Although it requires some expertise to use these algorithms
appropriately for the right tasks, there are many resources
online that make the learning curve easier.
Additionally the documentation on the website for
Scikit-learn includes tutorials to get started.
We find those documentation really easy to follow.
For example, the clustering part of the documentation
nicely overviews the available algorithms, their metrics,
scalability, parameters and even potential use cases.
Scikit-learn also includes specialized implementations
for dimensionality reduction algorithms.

Dimentionality Reduction:
->Enables you to reduce featureswhile preserving variance.
->scikit-learn has capabilities for:
  ->principal component Analysis(PCA)
  ->singular value decomposition
  ->Factor Analysis
  ->Independent Component Analysis
  ->Matrix factorization
  ->Latent Dirichled allocation(LDA)

Classification:
----------------
In a classification problem, the input data is presented to the machine learning model,
and the task is to predict the target corresponding to the input data.
The target is a categorical variable. So the classification task is to predict
the category or label of the target given the input data.
Eg: we have to predict weather b/w windy,sunny,rainy
==> Since a target is provided, we have label data,
and so classification is a supervised task.Recall that in a supervised task, the target,
or design output for each sample is given.Note that the target variable goes by many names
such as target, label, output,class variable, category, and class.
A classification problem can be binary,or multi-class.
With the binary classification, the target variable has two possible values, for example yes and no.
With multi-class classification, the target variable has more than two possible values, for example
the target can be short, medium, and tall.
Multi-class classification is also referred to as multinomial or multi-label classification.
Note:
1)Remember,the target is always a categorical variable in classification.
2)The target is provided for each sample,classification is a supervised task.

**In building a model, we need to adjust the parameters in order to reduce the model's error.
==> we need to form decision boundaries.
In the case of supervised tasks, such as classification,
this means getting the model's outputs to match the targets,
or desired outputs, as much as possible.

Building a classification model then means using the data
to adjust a model's parameters in order to form decision
boundaries to separate the target classes.(eg: in a graph we can say that all the 
elements falling in area 1 are squares,area 2 are circles etc hence
the area boundries of there areas(1 and 2) are nothing but decision boundaries) 
Note that the term classifier is often used to mean classification model.
Training phase: train data + learning Algotithm = build model ==> o/p = model
Testing phase: test data +model = apply model ==> o/p result
To adjust a model's parameters, we need to apply a learning algorithm.
commonly used algorithms for building a classification model:
kNN or k-nearest neighbors(kNN stands for K-nearest Neighbors),
Decision Trees
and NaÃ¯ve Bayes.
KNN:
This technique relies on the notion that samples
with similar characteristics, that is samples with similar
values for input, likely belong to the same class.
So classification of a sample is dependent
on the target values of the neighboring points.
Decision Tree:
A Decision Tree is a classification model that uses a tree-like structure to represent multiple decision paths.
Traversing each path leads to a different way to classify an input sample.
Eg:
               |---->non mamals
warm blooded---|                  |-----> non mammals
               |---->live birth --|                    |----->Non-Mammals
                                  |----->vertebrate ---|
                                                       |----->Mammals
According to this decision tree,if an animal is warm-blooded, gives live birth,
and has a vertebrae, then it is a mammal.
If an animal does not have any of these three characteristics, then it's not a mammal.                                                       
Naive Bayes:
NaÃ¯ve Bayes model uses a probabilistic approach to classification.
Bayes' Theorem is used to capture the relationships within the input data and the output class.
Bayes Theorem:
         P(B|A) * P(A)
P(A|B)= ---------------
            P(B)
The Bayes Theorem compares the probability of an event in the 
presence of another event.Here we see the probability of A if B is present.
An example for this is probability of having a fire if the weather is hot.
You can imagine event B depending on more than one variable,
e.g. weather is hot and windy.

Decision Tree Models:
The idea behind decision trees for classification
is to split the data into subset,where each subset belongs to only one class.
This is accomplished by dividing the input space into pure regions.
That is, regions with samples from only one class.
With real data, completely pure subsets may not be possible,
so the goal is to divide the data into subsets that are as pure as possible.
That is, each subset contains as many samples as possible of a single class.
Graphically, this is equivalent to dividing the input space
into regions that are as pure as possible.Boundaries separating these regions
are called decision boundaries,and the decision tree model makes classification decisions
based on these decision boundaries.

A decision tree is a hierarchical structure with nodes and directed edges.
The node at the top is called a root node.The nodes at the bottom are called leaf nodes.
Nodes that are neither the root node or the leaf nodes are called internal nodes.
The root and internal nodes have test conditions.
Each leaf node has a class label associated with it.
A classification decision is made by traversing the decision tree, 
starting with the root node.At each node, the answer to the test condition
determines which branch to traverse to.When a leaf node is reached, 
the category at the leaf node determines the classification decision.

The depth of a node is the number of edges from the root node to that node.
The depth of the root node is zero.The depth of a decision tree is the number of edges
in the longest path from the root node to the leaf node.
The size of a decision tree is the number of nodes in the tree.
==> above mammal example has tree depth= 3 and tree size = 6

Creating a Decision tree:
At high level,constructing a decision tree consists of following steps.
Start with all samples at a node.
Partition the samples into subsets based on the input variables.
Here, the goal is to create subsets of records that are purest.
That is, each subset contains as many samples as possible
belonging to just one class.Another way to say this is that the subsets
should be homogenous, or as pure as possible.
Repeat to partition data into successively pure subsets
until stopping criteria are satisfied.
And an algorithm for constructing a decision tree model
is referred to as induction algorithm,
so you may hear the term tree induction
used to describe the process of building a decision tree.
Note that at each split,the induction algorithm only considers the best way
to split the particular portion of the data.
This is referred to as a greedy approach.

Greedy algorithms solve a subset of the problem at the time,
and is a necessary approach when solving the entire problem is not feasible.
And by feasible, I mean computable in a reasonable amount of time or space.
Using a greedy algorithm is necessary for decision trees.

It is not feasible to determine the best tree given a dataset,
so the tree has to be built in a piecemeal fashion by determining
the best way to split the current node at each step,and combining
these decisions together to form a final decision tree.
we need a way to measure the purity of a split
in order to compare different ways to partition a set of data.
It turns out that it works better mathematically
if you measure the impurity,rather than the purity, of a split.
So the impurity measure of a node specifies how mixed the resulting subsets are.
Since we want the resulting subsets to have homogenous class labels,
not mixed class labels,we want the split that minimizes the impurity measure.
i.e when a node is split ==> we need to make sure that split is pure.
    hence we have a measure to check the purity of that split.
    
A common impurity measure used for determining
the best split is Gini index.The lower the Gini index,
the higher the purity of the split, so the decision 
tree will select the split that minimizes the Gini index.
Besides Gini index, other impurity measures include
entropy/information gain and misclassification rate.
*The other factor in determining the best way
to partition a node is which variable to split on.
The decision tree will test all variables
to determine the best way to split the nodes,
using a purity measure such as the Gini index
to compare the various possibilities.
Recall that the tree induction algorithm repeatedly
splits nodes to get more and more homogenous datasets.

So when does this process stop building subsets?
When does the algorithm stop growing the tree?
There are several criteria that can be used to determine
when a node should no longer be split into subsets.
-->The induction algorithm can stop expanding a node
when all samples in the node have the same class label.
This means that this set of data is as pure as possible,
and further splitting will not result
in any better partition of the data.
And since getting completely pure subsets
is difficult to achieve with real data,
this stopping criterion can be modified
to when a certain percentage of the samples in the node,
say 90%, for example, have the same class labels.
-->The algorithm can stop expanding a node when the number
of samples in the node falls below a certain minimum value.
At this point, the number of samples is too small
to make much difference in the classification results
with further splitting.
-->The induction algorithm can also stop expanding a node
when the improvement in impurity measure is too small
to make much of a difference in classification results.
-->Additionally, the tree, or the algorithm, can stop expanding
a node when the maximum tree depth is reached.
This is to control the complexity of the resulting tree.
-->There can be other criteria that can be used
to determine when tree induction should stop.
Therefore typically we have 4 points to stop a tree from expanding further i.e
-->All(or X% of) samples have same class label.
-->no. of samples in node reaches a minimum value.
-->change in impurity measure is smaller than the threshold.
-->max tree depth is reached.

Eg:
Let's say we want to classify loan applicants as being
likely to repay a loan, or not likely to repay a loan,
based on their income and amount of debt they have.
Building a decision tree for this classification problem
could proceed as follows:

One way to split this dataset into a more homogenous subset
is to consider the decision boundary where income is t1.
To the right of this decision boundary are mostly red(repayers)
samples, and to the left are mostly blue samples(defaulters).
The subsets are not completely homogenous,
but that is the best way to split the original dataset
based on the variable income.
The decision tree boundary is represented
in the decision tree by the condition
income is greater than t1 at the root node.
Samples with income greater than the threshold value of t1
are placed in the right subset, and samples with income
less than or equal to t1 are placed in the left subset,
just as shown in the left diagram.
Because the right subset almost perfectly predicts
that lenders will repay properly,
the right subset is now labeled as red.
The second step, then, is to determine
how to split the left region.
the best way to split this data is specified
(***we draw a Debt vs Income graph and we create the boundaries manually by seeing the graph)
by the second decision boundary, with debt equals t2(threshold 2).
Now the right region contains all blue samples,
and so the corresponding node is labeled blue,
meaning that the loan applicant
is not likely to repay the loan.
The third and final split looks at how to split
the region outlined in red in the diagram(graph Debt vs Income).
The best split is specified by the boundary with income equals t3.
This splits the red region into two pure subsets.
The split is represented in the decision tree
by adding a node with condition income is greater than t3,
and the left resulting node is labeled blue,
and the right resulting node is labeled red,
corresponding to the resulting subsets
with the red border in the diagram(graph)


             |--->Red(contains all red samples)
income >T1---|               |--->Blue(contains all blue samples)
             |--->Debt >T2---|                 |--->Red(contains all red samples)
                             |--->income >T3---|
                                               |--->Blue(contains all Blue samples)
                                               
You may have noticed that the decision boundaries of a decision tree are parallel
to the axes formed by the variables.(in graph)
This is referred to as being rectilinear.
The boundaries are rectilinear because each split
considers only a single variable.
There are variants of the tree induction algorithm
that consider more than one attribute
when splitting a value.
However, each split has to consider all combinations
of combined variables, and so such induction algorithms
are more computationally intensive,
or we can also call them more computationally expensive.

There are a few important things to note
about the decision tree classifier.
The resulting tree is often simple to understand
and interpret, and this is one of the biggest advantages
of decision trees for classification.
It is often possible to look at the resulting tree
to see which variables are important
to the classification problem,
and understand how the classification is performed.
For this reason,many people will start out with a decision
tree classifier to get a feel for the classification problem, even if
they end up using a more sophisticated model later on.
The tree induction algorithm, as described in this lesson,
is relatively computationally inexpensive,
so training a decision tree for classification
can be relatively fast.

Note:The greedy approach used by the tree induction algorithm
determines the best way to split the proportion
of the data at a node, but does not guarantee
the best solution overall for the entire dataset.

Decision boundaries are rectilinear,
and this can limit the expressiveness
of the resulting model, which means it may not be able
to solve complicated classification problems
that require more complex decisions,
or decision boundaries, to be formed.

In summary, the decision tree classifier
uses a tree-like structure to specify a series of conditions
that are tested to determine the class label for a sample.
The decision tree is constructed by repeatedly
splitting the data, and partitioning the data
into successively more homogenous subsets.

Live code Decision Tree:(weather classification using decission trees)
-----------------------------------------------------------------------
we will perform a decision tree base classification of metadata.
This metadata was curated from sensor data
of a weather station located in San Diego, California
and the weather station has sensors that capture
weather related measurements such as air temperature,
air pressure, and relative humidity.
The data was collected from a period of three years.
So it has longitudinal information from September 2011
to September 2014 and this ensures also
that sufficient data for different seasons
and weather conditions are captured.

>>>import pandas as pd
>>>from sklearn.metrics import accuracy_score
>>>from sklearn.model_selection import train_test_split
>>>from sklearn.tree import DecisionTreeClassifier
The task of classification,is a supervised learning task
where we supervise the algorithm to learn how to label a given 
sample by training it.
Here we will leverage existing methods in the scikit library.
>>>data = pd.read_csv('./weather/daily_weather.csv')
#cross check the data
>>>data.columns   #see the columns
>>>data           #see the actual data
Below is the columns(total 11 columns) descriptions
Each row, or sample, consists of the following variables:

1>number: unique number for each row
2>air_pressure_9am: air pressure averaged over a period from 8:55am to 9:04am (Unit: hectopascals)
3>air_temp_9am: air temperature averaged over a period from 8:55am to 9:04am (Unit: degrees Fahrenheit)
4>air_wind_direction_9am: wind direction averaged over a period from 8:55am to 9:04am (Unit: degrees, with 0 means coming from the North, and increasing clockwise)
5>air_wind_speed_9am: wind speed averaged over a period from 8:55am to 9:04am (Unit: miles per hour)
6>max_wind_direction_9am: wind gust direction averaged over a period from 8:55am to 9:10am (Unit: degrees, with 0 being North and increasing clockwise)
7>max_wind_speed_9am: wind gust speed averaged over a period from 8:55am to 9:04am (Unit: miles per hour)
8>rain_accumulation_9am: amount of rain accumulated in the 24 hours prior to 9am (Unit: millimeters)
9>rain_duration_9am: amount of time rain was recorded in the 24 hours prior to 9am (Unit: seconds)
10>relative_humidity_9am: relative humidity averaged over a period from 8:55am to 9:04am (Unit: percent)
11>relative_humidity_3pm: relative humidity averaged over a period from 2:55pm to 3:04pm (Unit: percent )

Checkif the data has any null values
>>>data[data.isnull().any(axis=1)] #we can see NaN values
Now lets clean data:
We will not need number for each row so we can clean it.
>>>del data['number']
Now let's drop null values using the *pandas dropna* function.
>>>before_rows = data.shape[0]
>>>print(rows before dropping null values)
>>>data = data.dropna()
>>>after_rows = data.shape[0]
>>>print(after_rows)
We will use a classifier to predict humidity on a given
afternoon by looking at the weather in the morning.

Remember we use the dropna function
to generate a slice of the data and assign that
to the DataFrame instead(above).Now we'll copy 
the slice into a new DataFrame that is not a slice.
It's a real DataFrame or what we would call,
the data is what we would call a view
over the original data set, the one that we assign it
we'll call this clean data will be real DataFrame again.
(i.e whn we copy the data after dropna fn it is similar to a view in DB)
>>>clean_data = data.copy()
creating a cew column in the new clean Data frame
**i.e Binarize the relative_humidity_3pm to 0 or 1
and we label our target label as high_humidity_label
>>>clean_data['high_humidity_label'] = (clean_data['relative_humidity_3pm'] > 24.99)*1
>>>print(clean_data['high_humidity_label'])
This new column will store 1 for relative_humidity_3pm 
values higher than 24.99,else they store 0.
**Bascically it gives us a true/false value.
We multiply this true/false value generated 
by that comparison greater than operator by 1
to turn the column into integer values, 0 or 1.
Going with the parametric function analogy,
y = f(x) we will now create a new DataFrame called y
for what we are trying to predict.
So by looking at x we'll try to predict y,
that's the analogy.And we will do this
by storing our High_Humidity_Label column into y.
>>>y=clean_data[['high_humidity_label']].copy()
print it and check whtit stored
>>>y
we can also print out the first five rows of y
and first 5 rows of clean data to compare the
humidity values with the High Humidity Labels and 
cross check them.
>>>clean_data['relative_humidity_3pm'].head()
>>>y.head()
==> 1 in y values means corresponding value in clean data is
higher than 24.99 
Now we'll select the features of the data in the morning.
==>load all these morning 9 am variables
or features into a list called Morning Features.
>>>morning_features = ['air_pressure_9am','air_temp_9am','avg_wind_direction_9am','avg_wind_speed_9am',
>>>        'max_wind_direction_9am','max_wind_speed_9am','rain_accumulation_9am',
>>>        'rain_duration_9am']
After we run this we will create a matrix or a DataFrame
in this case with only these morning features as columns.
We will literally copy the values
in those morning-related columns into a DataFrame called x.
Remember again we are trying to do y = fx.
>>>X = clean_data[morning_features].copy()
So given the x we'll try to come up with the y values.
That's what we are trying to do.
Here the values in clean data are now in x
are pretty standard data types.
However, if they were lists or other more complex data types
we might have had to use a deep copy.
**In that case we would have said inside those parentheses
in the copy 'deep = true' for the copy function
that will ensure if you have any complicated data types
like arrays or lists in your input matrix
they'll be copied over to x in this situation. i.e copy(deep = trye)

Now we will start building a model, our first model,
a classification model using Decision Trees.
To do that we will split the data set
into tests and training samples.
To do this,we use train test split function
to generate these two data sets.
Now these two data sets
will be slices of our original data set stored in x
and labels stored in y.
>>>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
Let's discuss a little bit the arguments here.
We have the train test split, we give it the x, our input,
and y, the labels for training.
i.e for these values of X(morning weather) we got these values of Y(afternoon weather)
In addition to these 2 we provide a value
for how much of the data we want to test with
called the test size.
==>we are telling this function to store 33% of the data,
keep it, don't show it to the trainer
as we'll use it for testing later.
We also have the random state seed here set as 324.
Although the seed is changeable
it will lead to different partitioning
of the original data set
and later leading to different prediction accuracy.
(we can change seed randomly to get different accuracy)

Therefore,Train test split here takes two DataFrames
and returns four DataFrames, right?
It's the x train, x test, y train, and y test.

Have a look of data and the data types
#type(X_train)
#type(X_test)
#type(y_train)
#type(y_test)
#X_train.head()
#y_train.describe()

Now let's use a decision tree classifier
to train on the training data set.
and we will name this training data set 
(our decision tree in that sense)as humidity classifier.
>>>humidity_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)
>>>humidity_classifier.fit(X_train, y_train)
==> we have created a classifier and applied/fitted our train data to it.
we have set max leaf nodes to 10.
So this is our stopping criteria for the tree induction.
If you left it as default it would have been unlimited,
making it potential over fit the tree to the training data.
We don't want that
but in general you need to experiment with it a little bit.
And the random state argument in this algorithm
is used for splitting the nodes.
Zero here is one of the internal states
used to build a decision tree.
Although, just like random seed before,
this one's also can have random values.
Hence we created a decision tree classifier object
called humidity classifier, and we have used fit() method of this 
object to make the classifier tune itself
to learn from the samples.
==> owr model is built now
>>>type(humidity_classifier)
o/p :sklearn.tree.tree.DecisionTreeClassifier
i.e it is of type decision tree clasifier
other arguments of decisionTreeClasifier () are as below:
(below are are the arguments our fn took)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=10, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=0,
            splitter='best')

Now we will use the test data to test the model we have built
We will use the predict function of the decision tree
classifier object which was our humidity classifier.
This function will accept our test data.
Let's run this and display the first 10 values
in those predictions 
>>>predictions = humidity_classifier.predict(X_test)
>>>type(predictions)
o/p it is an numpy array
>>>predictions[:10]
==> o/p : array([0, 0, 1, 1, 1, 1, 0, 0, 0, 1])
therefore these predictions array is out predicted labels for the test data of X.
Hence,now we compare this predicted values with already
existing labels of test data of X i.e test data of Y.
==> comapring first 10 values predicted with first 10 values of y_test
>>>y_test['high_humidity_label'][:10]
o/p :
     0
     0
     1
     1
     1
     1
     1
     0
     1
     1
==> we can see that there are some errors i.e 7th value 
of predictions is '0' while 7th valuue of y_test is '1'            
lly 9th and 10th values are also different.
For the classification task an error occurs, just like this,
when the model's prediction of the class label
is different from the true class label.
We can measure these errors by calculating
the number of correct labels just like I've done now.
We had 3 wrong, so if we had 10 rows
it would be that many wrong in it
and we can turn it into an error measure.
but we can't count always right? as the data might be huge sometimes.
Therefore we have a fn to check the acuraccy of the model.
>>>accuracy_score(y_true = y_test, y_pred = predictions)
o/p:0.81534090909090906
It's called accuracy score and if you look at that,
the accuracy of our algorithm here is about 81%.
We can also define the different types of error
in the classification depending of the predicted
and true labels.

------------------------------------------------------------------------------------
CLUSTERING(It is an unsupervised task):
---------------------------------------
A very common application of cluster analysis
that we have discussed before is to divide 
your customer base into segments
based on their purchase histories.Some other 
examples of cluster analysis are;characterization
of different weather patterns for a region,
grouping the latest new articles into topics
to identify the trending topics of the day,
and discovering hot spots for different types of crime
from police reports in order to provide sufficient
police presence for problem areas.

Cluster analysis divides all the samples in a data set
into groups.The goal of cluster analysis is to segment data
so that the differences between samples in the same
cluster are minimized.And the differences between samples of
different clusters are maximized.
Cluster analysis requires some sort of metric
to measure similarity between two samples.
Some common similarity measures are:

A)Euclidean distance, which is the distance along
a straight line between two points, A and B.
B)Manhattan distance, which is calculated on a strictly
horizontal and vertical path,you can only step along 
either the X axis or the Y axis
in the two-dimensional case.
So the path to calculate the Manhattan distance
crosses two segments along the axis,
instead of along a diagonal path
as with the Euclidean distance.
C)Cosine similarity measures the Cosine of the angle
between points A and B.
Since distance measures such as Euclidean distance
are often used to measure similarity between samples
in clustering algorithms, note that it may be necessary
to normalize the input variables so that no one value
dominates the similarity calculation.
As we have learnt about scaling and normalizing earlier.
**Normalizing is one method to scale variables.
Essentially scaling with input variables
puts the variables on the same scale so that all variables
have equal weighting in the calculation to determine
similarity between samples.
Scaling is necessary when you have variables that have
very different scales, such as weight and height.
The magnitude of the weight values which are in pounds,
will be much larger than the magnitude of the height values
which are in feet and inches.
So scaling both variables to a common value range
will make the contributions
from both weight and height equal.

Things to note about cluster analysis are:
first, unlike classification and regression in general,
cluster analysis is an unsupervised task.
This means that there is no target label for any sample
in the data set.
In general, there is no correct clustering results.
The best set of clusters is highly dependent
on the application,
how the resulting clusters will be used.
There are numerical measures to compare
two different clusters,but since there are no
labels to determine whether a sample has been correctly 
clustered,there's no grounds to determine if a
set of clustering results are truly correct or
incorrect.Clusters don't come with labels.You may end
up with five different clusters at the end of cluster 
analysis process, but you don't know what each cluster 
represents.Only by analyzing the samples in each 
cluster further,you can come up with reasonable labels
for your clusters.
***It is important to keep in mind
that interpretation and analysis of the clusters
are required to make sense of and make use of the
results of cluster analysis.
There are several ways that the results of cluster analysis
can be used.
The most obvious is data segmentation
Clusters can also be used to classify new data samples.(scince elements,customers etc based on properties).
Once cluster labels have been determined(say child,young,old groups are labels)
samples in each cluster can be used as label data
for another classification task.
The samples would be the input to the classification model,
and the cluster label will be the target class
for each sample.
Another use of cluster results is
as a basis for anomaly detection.
If a sample is very far away or very different
from any of the cluster centers/groups/segmnts,
then that sample is a cluster outlier and can be flagged
as anomaly.However, these anomalies require further analysis.
Depending on the application,these anomalies can be considered noise
and should be removed from the data set.
An example of this would be a sample with value of 150
for age.
For other cases, these anomalies cases should be studied
more carefully.
Examples of this are in a credit card fraud detection
or network intrusion detection application.
To summarize: cluster analysis is used to organize
similar data items into groups or clusters.
Analyzing the resulting clusters often leads to
useful insights about the characteristics of each group
as well as the underlying structure of the entire data set.
Clusters require analysis and interpretation
to make sense of the results since there are no labels
associated with samples or clusters in a clustering task.

K-Means Clustering:
--------------------
By the end of this topic, you should be able to:
describe the steps in the k-means algorithm,
explain what the k stands for in k-means,
and define what the cluster centroid is.
K-means is a classic algorithm used for cluster analysis.
The algorithm is very simple.
->The first step is to select k initial centroids.(any point in a
graph)( and graph plotted for data of samples for which clutering is being done)
***no. of initial centroid = no. of clusters at last
Therefore K = no. of clusters
A centroid is simply the center of a cluster.
->Next, assign each sample in a data set
to the closest centroid.
This means you calculate the distance
between the sample and each cluster center
and assign the sample to the cluster
with the closest centroid.
->Then you calculate the mean or average of each cluster
to determine a new centroid.
First two steps are done repeatedly
until some stopping criteria is reached.

Note:the final clusters are sensitive to initial centroids.
This means that if cluster results with one set
of initial centroids can be different from results
with another set of initial centroids.

How are initial centroids selected?
There are many approaches to selecting
the initial centroids for k-means
varying in levels of sophistication.
The easiest and most widely used approach
is to apply k-means several times with different
initial centroids randomly chosen to cluster your data set
and then select the centroids that give you
the best clustering results.

To evaluate the cluster results an error measure known
as the Within-Cluster Sum of Squared Error can be used.
The error associated with a sample within a cluster
is the distance between the sample and the cluster centroid.
The squared error for the sample then is the square of that distance.
i.e error = dist b/w sample and centroid
    squared error = error*error
We then do the same thing for all clusters
to get the final calculation
for the Within-Cluster Sum of Squared Error(WSSE)
for all clusters in the results of a cluster analysis run.    
i.e if we have 4 clusters ==> we will have 4 WSSE's
Say if we have two clustering results(WSSE1 and WSSE2),
the one with the smaller WSSE provides the better solution numerically.
i.e if WSSE1<WSSE2 ==> WSSE1 is better
Note :1)This does not cluster set 1 is more correct that cluster set 2
      2)Larger values for K will always reduce WSSE
Therefore, as we've discussed before, there's no ground truth
to mathematically determine which set of clusters
is more correct than the other.
In addition, note that increasing the number of clusters
that is a value for k always reduces the WSSE.
So WSSE should be used with caution.
It only makes sense to use WSSE to compare two sets
of clusters with the same value for k
and generated from the same data set.

There are several methods to determine the value for k
-->Visualization techniques can be used to examine the
data set to see if there are natural groupings of the samples.
Scatterplots and the use of dimensionality reduction
are useful here to visualize the data.
-->A good value for k is application dependent (method).
So domain knowledge of the application can drive
the selection for the value of k.
-->There are also Data-Driven methods
for determining the value of k.
These methods calculate some metric for different values
of k to determine the best selection for k.
One such method is the Elbow method.
The Elbow method for determining the value of K.
If you plot WSSE for different values for k,
we can see how this error measure changes
as the value of k changes as seen in the plot.
The bend in this error curve indicates a drop
in gain by adding more clusters.
This Elbow in the curve provides a suggestion
for a good value of k.
X axis = k(no. of clusters)
Y axs = WSSE results
Note that the elbow cannot
always be unambiguously determined
especially for complex data.
In many cases, the error curve will not have
a clear suggestion for one value but multiple values.
This can be used as a guideline
for the range of values to try for k.

Let's now look at when to stop.
How do you know when to stop iterating
when using k-means?
->One obvious stopping criteria is when
there are no changes to the centroids.
This means that no samples will change cluster assignments,
and re-calculating the centroids
will not result in any changes.
Additional iterations will not bring any more changes
to the cluster results.It's time to stop.
->The second stopping criteria is when the number of 
samples change in clusters is below a certain threshold,
say one percent for example.At this point, the clusters 
are changing only by a few samples resulting in only 
minimal changes to the final clustering results.
The algorithm can be stopped here as well.

Interpreting Results:
Each centroid is the mean of the samples
assigned to that cluster.
You can think of the centroid
as a representative sample for that cluster.
So to interpret the cluster analysis results,
we can examine the cluster centroids.
Comparing the values of the variables between the centroids
will reveal how different or alike clusters are
and provide insight into what each cluster represents.

Live code:Clustering:
-----------------------------------------


