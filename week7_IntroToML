Machine learning: is a field of study that focuses on computer systems that can learn from data.
That is, these systems, often called models,can learn to perform a specific task
by analyzing lots of examples for a particular problem.

Eg:identify a cat after analysisng lots of image of cats
The Machine Learning algorithm is programmed to learn from the data that 
there's nothing in the algorithm or program which directly aims to learn the given task.
** i.e Machine Learning models are not given the step by step instructions on how to recognize
the image of a cat.Instead, the model learns on its own what features
are important in determining that a picture contains a cat from the data that it has analyzed.

*Because the model learns to perform this task from data,it's good to note that the amount and quality of data
available for building the model are important factors in how well the model learns from the task.

Therefore,Machine Learning models learn from data to perform a task
without being explicitly programmed.

Clasification:
In classification,the goal is to predict the category of the input data.
The categories to be predicted are called classifications.
Eg:predicting the weather as being sunny,rainy, windy or cloudy.
If the predicted output is b/w 2 values ==> it is called binary Classification.

Regression:
When your model has to predict a numeric value instead of a category,
then the task becomes a regression problem.
Eg:predict the price of a stock.
The stock price is a numeric value,not a category, so this is a regression task.
If you were to predict whether the stock price will rise or fall,then that would 
be a classification problem but if you're predicting the actual price of the stock,
then that's a regression problem.

To summarize,in classification you're predicting a category,
and in regression,you're predicting a numeric value.

Clustering:
In cluster analysis,the goal is to organize similar items in your dataset into groups.
A very common application of cluster analysis is referred to as customer segmentation.
This means that you're separating your customer base into different groups or segments
based on customer types.
For example,
it would be beneficial to segment your customers into seniors, adults and teenagers.

association analysis:
The goal here is to come up with a set of rules
to capture associations between items or events.
The rules are used to determine when items or events occur together.
A common application of association analysis is known as market basket analysis,
which is used to understand customer purchasing behavior.
For example,
->association analysis can reveal that banking customers
who have checking or deposit accounts also tend to be interested 
in other investment vehicles such as money market accounts.
->Other common applications can be recommendation of similar items
based on purchasing or browsing history of customers.
->Identification of web pages that are often accessed together
can also provide us a good basis for association analysis.

For the techniques we've discussed here,there are two ways of conducting 
the learning itself.These categories are referred to as 
A)supervised Learning and B)unsupervised Learning

In supervised approaches,(Target is available)
the target, which is what the model is predicting,is provided.
This is referred to as having labeled data because the target is 
labeled for every sample that you have in your dataset.
Referring back to our example of predicting the weather category
of sunny, windy, rainy or cloudy,every sample in the dataset
is labeled as being one of these four categories.
So, the data is labeled and predicting the weather category is a supervised task.
**In general,classification and regression are supervised approaches.

In unsupervised approaches,(Target is not available)
on the other hand,the target that the model is predicting
is unknown or unavailable.This means that you have unlabeled data,
so you can't train using these labels.Going back to our cluster 
analysis example of segment customers into different groups,the samples
in your data are not labeled with the correct group.
Instead, the segmentation is performed using a clustering technique
to group items based on characteristics of what they have in common.
Thus, the data is unlabeled and the task of grouping customers
into different segments is an unsupervised one.
**In general, cluster analysis and association analysis are unsupervised approaches.

Smple defination for easy understanding:
----------------------------------------
Supervised Learning:
Supervised learning is the machine learning task of inferring 
a function from labeled training data. The training data consist
of a set of training examples. In supervised learning, each example
is a pair consisting of an input object (typically a vector) and a 
desired output value (also called the supervisory signal)
Since in supervised learning in the training set we hav the values for target
i.e we will have the data which states:"for these inuts we had these outputs"
Unsupervised learning:
is a type of machine learning algorithm used to draw inferences
from datasets consisting of input data without labeled responses.
The most common unsupervised learning method is cluster analysis,
which is used for exploratory data analysis to find hidden patterns or grouping in data.
--------------------------
==> Regression and classification are a supervised learning task
==> in case of a supervised learnigntask we will have
a Test data set and a Train data set and we built a model and test it.

Training Data:
Used to adjust Model parameters

Test Data:
Evaluate performance on new data

there is also something called Valdation data
Validation data:
The validation dataset
is used to determine when to stop the 
training in order to avoid model overfitting.

Terminology in ML:
Sample:
It is an instance/example of an entity in your data.It is typically a row in your dataset.

ID    Name     mat     phy 
1     A1        11      20
2     A2        14      28
3     A3        18      22

==> row's with ID's 1,2,3 are called Samples
and coloumns(ID,Name,Mat and Phy) are called Variables/features of sample.
and Each sample has 4 values(variables) associated with it.
We call these different values as variables of the sample
and sometimes refer to them as features of the sample.
A variable captures specific characteristics of each entity.

There are many names for variables in addition to feature.
like: feature, column,dimension, attribute and field etc.

A variable holding Numerical values is called as Numerical/quantitative variable.
A variable holding characters/strings ar called as categorial/nominal/qualitative variable.

Scikit-learn: 
->An open source ML library in Python
->It is built on top of NumPy, SciPy, and matplotlib like many other Python libraries.
->It can be used for end to end Machine Learning in python.
i.e entire data science process including Machine Learning,
data cleaning, and data transformations,.etc can be done by scikit-learn.

Scikit Learn Provides fns for preprocessing such as:
->fns for converting raw feature vectors into suitable formats
-> it providea API's for
  ->scaling of features: remove mean and keep unit variance
  ->Normalization to have unit form
  ->binarization to turn data into 0 or 1 form
  ->One hot encoding for categorial features
  ->handlin of missing values
  ->generating higher order features
  ->built custom transformations
Scikit-learn also provides built-in functions for many
Machine Learning algorithms ready for modeling and analysis.
Although it requires some expertise to use these algorithms
appropriately for the right tasks, there are many resources
online that make the learning curve easier.
Additionally the documentation on the website for
Scikit-learn includes tutorials to get started.
We find those documentation really easy to follow.
For example, the clustering part of the documentation
nicely overviews the available algorithms, their metrics,
scalability, parameters and even potential use cases.
Scikit-learn also includes specialized implementations
for dimensionality reduction algorithms.

Dimentionality Reduction:
->Enables you to reduce featureswhile preserving variance.
->scikit-learn has capabilities for:
  ->principal component Analysis(PCA)
  ->singular value decomposition
  ->Factor Analysis
  ->Independent Component Analysis
  ->Matrix factorization
  ->Latent Dirichled allocation(LDA)

Classification:
----------------
In a classification problem, the input data is presented to the machine learning model,
and the task is to predict the target corresponding to the input data.
The target is a categorical variable. So the classification task is to predict
the category or label of the target given the input data.
Eg: we have to predict weather b/w windy,sunny,rainy
==> Since a target is provided, we have label data,
and so classification is a supervised task.Recall that in a supervised task, the target,
or design output for each sample is given.Note that the target variable goes by many names
such as target, label, output,class variable, category, and class.
A classification problem can be binary,or multi-class.
With the binary classification, the target variable has two possible values, for example yes and no.
With multi-class classification, the target variable has more than two possible values, for example
the target can be short, medium, and tall.
Multi-class classification is also referred to as multinomial or multi-label classification.
Note:
1)Remember,the target is always a categorical variable in classification.
2)The target is provided for each sample,classification is a supervised task.

**In building a model, we need to adjust the parameters in order to reduce the model's error.
==> we need to form decision boundaries.
In the case of supervised tasks, such as classification,
this means getting the model's outputs to match the targets,
or desired outputs, as much as possible.

Building a classification model then means using the data
to adjust a model's parameters in order to form decision
boundaries to separate the target classes.(eg: in a graph we can say that all the 
elements falling in area 1 are squares,area 2 are circles etc hence
the area boundries of there areas(1 and 2) are nothing but decision boundaries) 
Note that the term classifier is often used to mean classification model.
Training phase: train data + learning Algotithm = build model ==> o/p = model
Testing phase: test data +model = apply model ==> o/p result
To adjust a model's parameters, we need to apply a learning algorithm.
commonly used algorithms for building a classification model:
kNN or k-nearest neighbors(kNN stands for K-nearest Neighbors),
Decision Trees
and Naïve Bayes.
KNN:
This technique relies on the notion that samples
with similar characteristics, that is samples with similar
values for input, likely belong to the same class.
So classification of a sample is dependent
on the target values of the neighboring points.
Decision Tree:
A Decision Tree is a classification model that uses a tree-like structure to represent multiple decision paths.
Traversing each path leads to a different way to classify an input sample.
Eg:
               |---->non mamals
warm blooded---|                  |-----> non mammals
               |---->live birth --|                    |----->Non-Mammals
                                  |----->vertebrate ---|
                                                       |----->Mammals
According to this decision tree,if an animal is warm-blooded, gives live birth,
and has a vertebrae, then it is a mammal.
If an animal does not have any of these three characteristics, then it's not a mammal.                                                       
Naive Bayes:
Naïve Bayes model uses a probabilistic approach to classification.
Bayes' Theorem is used to capture the relationships within the input data and the output class.
Bayes Theorem:
         P(B|A) * P(A)
P(A|B)= ---------------
            P(B)
The Bayes Theorem compares the probability of an event in the 
presence of another event.Here we see the probability of A if B is present.
An example for this is probability of having a fire if the weather is hot.
You can imagine event B depending on more than one variable,
e.g. weather is hot and windy.

Decision Tree Models:
The idea behind decision trees for classification
is to split the data into subset,where each subset belongs to only one class.
This is accomplished by dividing the input space into pure regions.
That is, regions with samples from only one class.
With real data, completely pure subsets may not be possible,
so the goal is to divide the data into subsets that are as pure as possible.
That is, each subset contains as many samples as possible of a single class.
Graphically, this is equivalent to dividing the input space
into regions that are as pure as possible.Boundaries separating these regions
are called decision boundaries,and the decision tree model makes classification decisions
based on these decision boundaries.

A decision tree is a hierarchical structure with nodes and directed edges.
The node at the top is called a root node.The nodes at the bottom are called leaf nodes.
Nodes that are neither the root node or the leaf nodes are called internal nodes.
The root and internal nodes have test conditions.
Each leaf node has a class label associated with it.
A classification decision is made by traversing the decision tree, 
starting with the root node.At each node, the answer to the test condition
determines which branch to traverse to.When a leaf node is reached, 
the category at the leaf node determines the classification decision.

The depth of a node is the number of edges from the root node to that node.
The depth of the root node is zero.The depth of a decision tree is the number of edges
in the longest path from the root node to the leaf node.
The size of a decision tree is the number of nodes in the tree.
==> above mammal example has tree depth= 3 and tree size = 6

Creating a Decision tree:
At high level,constructing a decision tree consists of following steps.
Start with all samples at a node.
Partition the samples into subsets based on the input variables.
Here, the goal is to create subsets of records that are purest.
That is, each subset contains as many samples as possible
belonging to just one class.Another way to say this is that the subsets
should be homogenous, or as pure as possible.
Repeat to partition data into successively pure subsets
until stopping criteria are satisfied.
And an algorithm for constructing a decision tree model
is referred to as induction algorithm,
so you may hear the term tree induction
used to describe the process of building a decision tree.
Note that at each split,the induction algorithm only considers the best way
to split the particular portion of the data.
This is referred to as a greedy approach.

Greedy algorithms solve a subset of the problem at the time,
and is a necessary approach when solving the entire problem is not feasible.
And by feasible, I mean computable in a reasonable amount of time or space.
Using a greedy algorithm is necessary for decision trees.

It is not feasible to determine the best tree given a dataset,
so the tree has to be built in a piecemeal fashion by determining
the best way to split the current node at each step,and combining
these decisions together to form a final decision tree.
we need a way to measure the purity of a split
in order to compare different ways to partition a set of data.
It turns out that it works better mathematically
if you measure the impurity,rather than the purity, of a split.
So the impurity measure of a node specifies how mixed the resulting subsets are.
Since we want the resulting subsets to have homogenous class labels,
not mixed class labels,we want the split that minimizes the impurity measure.
i.e when a node is split ==> we need to make sure that split is pure.
    hence we have a measure to check the purity of that split.
    
A common impurity measure used for determining
the best split is Gini index.The lower the Gini index,
the higher the purity of the split, so the decision 
tree will select the split that minimizes the Gini index.
Besides Gini index, other impurity measures include
entropy/information gain and misclassification rate.
*The other factor in determining the best way
to partition a node is which variable to split on.
The decision tree will test all variables
to determine the best way to split the nodes,
using a purity measure such as the Gini index
to compare the various possibilities.
Recall that the tree induction algorithm repeatedly
splits nodes to get more and more homogenous datasets.

So when does this process stop building subsets?
When does the algorithm stop growing the tree?
There are several criteria that can be used to determine
when a node should no longer be split into subsets.
-->The induction algorithm can stop expanding a node
when all samples in the node have the same class label.
This means that this set of data is as pure as possible,
and further splitting will not result
in any better partition of the data.
And since getting completely pure subsets
is difficult to achieve with real data,
this stopping criterion can be modified
to when a certain percentage of the samples in the node,
say 90%, for example, have the same class labels.
-->The algorithm can stop expanding a node when the number
of samples in the node falls below a certain minimum value.
At this point, the number of samples is too small
to make much difference in the classification results
with further splitting.
-->The induction algorithm can also stop expanding a node
when the improvement in impurity measure is too small
to make much of a difference in classification results.
-->Additionally, the tree, or the algorithm, can stop expanding
a node when the maximum tree depth is reached.
This is to control the complexity of the resulting tree.
-->There can be other criteria that can be used
to determine when tree induction should stop.
Therefore typically we have 4 points to stop a tree from expanding further i.e
-->All(or X% of) samples have same class label.
-->no. of samples in node reaches a minimum value.
-->change in impurity measure is smaller than the threshold.
-->max tree depth is reached.

Eg:
Let's say we want to classify loan applicants as being
likely to repay a loan, or not likely to repay a loan,
based on their income and amount of debt they have.
Building a decision tree for this classification problem
could proceed as follows:

One way to split this dataset into a more homogenous subset
is to consider the decision boundary where income is t1.
To the right of this decision boundary are mostly red(repayers)
samples, and to the left are mostly blue samples(defaulters).
The subsets are not completely homogenous,
but that is the best way to split the original dataset
based on the variable income.
The decision tree boundary is represented
in the decision tree by the condition
income is greater than t1 at the root node.
Samples with income greater than the threshold value of t1
are placed in the right subset, and samples with income
less than or equal to t1 are placed in the left subset,
just as shown in the left diagram.
Because the right subset almost perfectly predicts
that lenders will repay properly,
the right subset is now labeled as red.
The second step, then, is to determine
how to split the left region.
the best way to split this data is specified
(***we draw a Debt vs Income graph and we create the boundaries manually by seeing the graph)
by the second decision boundary, with debt equals t2(threshold 2).
Now the right region contains all blue samples,
and so the corresponding node is labeled blue,
meaning that the loan applicant
is not likely to repay the loan.
The third and final split looks at how to split
the region outlined in red in the diagram(graph Debt vs Income).
The best split is specified by the boundary with income equals t3.
This splits the red region into two pure subsets.
The split is represented in the decision tree
by adding a node with condition income is greater than t3,
and the left resulting node is labeled blue,
and the right resulting node is labeled red,
corresponding to the resulting subsets
with the red border in the diagram(graph)


             |--->Red(contains all red samples)
income >T1---|               |--->Blue(contains all blue samples)
             |--->Debt >T2---|                 |--->Red(contains all red samples)
                             |--->income >T3---|
                                               |--->Blue(contains all Blue samples)
                                               
You may have noticed that the decision boundaries of a decision tree are parallel
to the axes formed by the variables.(in graph)
This is referred to as being rectilinear.
The boundaries are rectilinear because each split
considers only a single variable.
There are variants of the tree induction algorithm
that consider more than one attribute
when splitting a value.
However, each split has to consider all combinations
of combined variables, and so such induction algorithms
are more computationally intensive,
or we can also call them more computationally expensive.

There are a few important things to note
about the decision tree classifier.
The resulting tree is often simple to understand
and interpret, and this is one of the biggest advantages
of decision trees for classification.
It is often possible to look at the resulting tree
to see which variables are important
to the classification problem,
and understand how the classification is performed.
For this reason,many people will start out with a decision
tree classifier to get a feel for the classification problem, even if
they end up using a more sophisticated model later on.
The tree induction algorithm, as described in this lesson,
is relatively computationally inexpensive,
so training a decision tree for classification
can be relatively fast.

Note:The greedy approach used by the tree induction algorithm
determines the best way to split the proportion
of the data at a node, but does not guarantee
the best solution overall for the entire dataset.

Decision boundaries are rectilinear,
and this can limit the expressiveness
of the resulting model, which means it may not be able
to solve complicated classification problems
that require more complex decisions,
or decision boundaries, to be formed.

In summary, the decision tree classifier
uses a tree-like structure to specify a series of conditions
that are tested to determine the class label for a sample.
The decision tree is constructed by repeatedly
splitting the data, and partitioning the data
into successively more homogenous subsets.

Live code Decision Tree:(weather classification using decission trees)
-----------------------------------------------------------------------
we will perform a decision tree base classification of metadata.
This metadata was curated from sensor data
of a weather station located in San Diego, California
and the weather station has sensors that capture
weather related measurements such as air temperature,
air pressure, and relative humidity.
The data was collected from a period of three years.
So it has longitudinal information from September 2011
to September 2014 and this ensures also
that sufficient data for different seasons
and weather conditions are captured.

>>>import pandas as pd
>>>from sklearn.metrics import accuracy_score
>>>from sklearn.model_selection import train_test_split
>>>from sklearn.tree import DecisionTreeClassifier
The task of classification,is a supervised learning task
where we supervise the algorithm to learn how to label a given 
sample by training it.
Here we will leverage existing methods in the scikit library.
>>>data = pd.read_csv('./weather/daily_weather.csv')
#cross check the data
>>>data.columns   #see the columns
>>>data           #see the actual data
Below is the columns(total 11 columns) descriptions
Each row, or sample, consists of the following variables:

1>number: unique number for each row
2>air_pressure_9am: air pressure averaged over a period from 8:55am to 9:04am (Unit: hectopascals)
3>air_temp_9am: air temperature averaged over a period from 8:55am to 9:04am (Unit: degrees Fahrenheit)
4>air_wind_direction_9am: wind direction averaged over a period from 8:55am to 9:04am (Unit: degrees, with 0 means coming from the North, and increasing clockwise)
5>air_wind_speed_9am: wind speed averaged over a period from 8:55am to 9:04am (Unit: miles per hour)
6>max_wind_direction_9am: wind gust direction averaged over a period from 8:55am to 9:10am (Unit: degrees, with 0 being North and increasing clockwise)
7>max_wind_speed_9am: wind gust speed averaged over a period from 8:55am to 9:04am (Unit: miles per hour)
8>rain_accumulation_9am: amount of rain accumulated in the 24 hours prior to 9am (Unit: millimeters)
9>rain_duration_9am: amount of time rain was recorded in the 24 hours prior to 9am (Unit: seconds)
10>relative_humidity_9am: relative humidity averaged over a period from 8:55am to 9:04am (Unit: percent)
11>relative_humidity_3pm: relative humidity averaged over a period from 2:55pm to 3:04pm (Unit: percent )

Checkif the data has any null values
>>>data[data.isnull().any(axis=1)] #we can see NaN values
Now lets clean data:
We will not need number for each row so we can clean it.
>>>del data['number']
Now let's drop null values using the *pandas dropna* function.
>>>before_rows = data.shape[0]
>>>print(rows before dropping null values)
>>>data = data.dropna()
>>>after_rows = data.shape[0]
>>>print(after_rows)
We will use a classifier to predict humidity on a given
afternoon by looking at the weather in the morning.

Remember we use the dropna function
to generate a slice of the data and assign that
to the DataFrame instead(above).Now we'll copy 
the slice into a new DataFrame that is not a slice.
It's a real DataFrame or what we would call,
the data is what we would call a view
over the original data set, the one that we assign it
we'll call this clean data will be real DataFrame again.
(i.e whn we copy the data after dropna fn it is similar to a view in DB)
>>>clean_data = data.copy()
creating a cew column in the new clean Data frame
**i.e Binarize the relative_humidity_3pm to 0 or 1
and we label our target label as high_humidity_label
>>>clean_data['high_humidity_label'] = (clean_data['relative_humidity_3pm'] > 24.99)*1
>>>print(clean_data['high_humidity_label'])
This new column will store 1 for relative_humidity_3pm 
values higher than 24.99,else they store 0.
**Bascically it gives us a true/false value.
We multiply this true/false value generated 
by that comparison greater than operator by 1
to turn the column into integer values, 0 or 1.
Going with the parametric function analogy,
y = f(x) we will now create a new DataFrame called y
for what we are trying to predict.
So by looking at x we'll try to predict y,
that's the analogy.And we will do this
by storing our High_Humidity_Label column into y.
>>>y=clean_data[['high_humidity_label']].copy()
print it and check whtit stored
>>>y
we can also print out the first five rows of y
and first 5 rows of clean data to compare the
humidity values with the High Humidity Labels and 
cross check them.
>>>clean_data['relative_humidity_3pm'].head()
>>>y.head()
==> 1 in y values means corresponding value in clean data is
higher than 24.99 
Now we'll select the features of the data in the morning.
==>load all these morning 9 am variables
or features into a list called Morning Features.
>>>morning_features = ['air_pressure_9am','air_temp_9am','avg_wind_direction_9am','avg_wind_speed_9am',
>>>        'max_wind_direction_9am','max_wind_speed_9am','rain_accumulation_9am',
>>>        'rain_duration_9am']
After we run this we will create a matrix or a DataFrame
in this case with only these morning features as columns.
We will literally copy the values
in those morning-related columns into a DataFrame called x.
Remember again we are trying to do y = fx.
>>>X = clean_data[morning_features].copy()
So given the x we'll try to come up with the y values.
That's what we are trying to do.
Here the values in clean data are now in x
are pretty standard data types.
However, if they were lists or other more complex data types
we might have had to use a deep copy.
**In that case we would have said inside those parentheses
in the copy 'deep = true' for the copy function
that will ensure if you have any complicated data types
like arrays or lists in your input matrix
they'll be copied over to x in this situation. i.e copy(deep = trye)

Now we will start building a model, our first model,
a classification model using Decision Trees.
To do that we will split the data set
into tests and training samples.
To do this,we use train test split function
to generate these two data sets.
Now these two data sets
will be slices of our original data set stored in x
and labels stored in y.
>>>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
Let's discuss a little bit the arguments here.
We have the train test split, we give it the x, our input,
and y, the labels for training.
i.e for these values of X(morning weather) we got these values of Y(afternoon weather)
In addition to these 2 we provide a value
for how much of the data we want to test with
called the test size.
==>we are telling this function to store 33% of the data,
keep it, don't show it to the trainer
as we'll use it for testing later.
We also have the random state seed here set as 324.
Although the seed is changeable
it will lead to different partitioning
of the original data set
and later leading to different prediction accuracy.
(we can change seed randomly to get different accuracy)

Therefore,Train test split here takes two DataFrames
and returns four DataFrames, right?
It's the x train, x test, y train, and y test.

Have a look of data and the data types
#type(X_train)
#type(X_test)
#type(y_train)
#type(y_test)
#X_train.head()
#y_train.describe()

Now let's use a decision tree classifier
to train on the training data set.
and we will name this training data set 
(our decision tree in that sense)as humidity classifier.
>>>humidity_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)
>>>humidity_classifier.fit(X_train, y_train)
==> we have created a classifier and applied/fitted our train data to it.
we have set max leaf nodes to 10.
So this is our stopping criteria for the tree induction.
If you left it as default it would have been unlimited,
making it potential over fit the tree to the training data.
We don't want that
but in general you need to experiment with it a little bit.
And the random state argument in this algorithm
is used for splitting the nodes.
Zero here is one of the internal states
used to build a decision tree.
Although, just like random seed before,
this one's also can have random values.
Hence we created a decision tree classifier object
called humidity classifier, and we have used fit() method of this 
object to make the classifier tune itself
to learn from the samples.
==> owr model is built now
>>>type(humidity_classifier)
o/p :sklearn.tree.tree.DecisionTreeClassifier
i.e it is of type decision tree clasifier
other arguments of decisionTreeClasifier () are as below:
(below are are the arguments our fn took)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=10, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=0,
            splitter='best')

Now we will use the test data to test the model we have built
We will use the predict function of the decision tree
classifier object which was our humidity classifier.
This function will accept our test data.
Let's run this and display the first 10 values
in those predictions 
>>>predictions = humidity_classifier.predict(X_test)
>>>type(predictions)
o/p it is an numpy array
>>>predictions[:10]
==> o/p : array([0, 0, 1, 1, 1, 1, 0, 0, 0, 1])
therefore these predictions array is out predicted labels for the test data of X.
Hence,now we compare this predicted values with already
existing labels of test data of X i.e test data of Y.
==> comapring first 10 values predicted with first 10 values of y_test
>>>y_test['high_humidity_label'][:10]
o/p :
     0
     0
     1
     1
     1
     1
     1
     0
     1
     1
==> we can see that there are some errors i.e 7th value 
of predictions is '0' while 7th valuue of y_test is '1'            
lly 9th and 10th values are also different.
For the classification task an error occurs, just like this,
when the model's prediction of the class label
is different from the true class label.
We can measure these errors by calculating
the number of correct labels just like I've done now.
We had 3 wrong, so if we had 10 rows
it would be that many wrong in it
and we can turn it into an error measure.
but we can't count always right? as the data might be huge sometimes.
Therefore we have a fn to check the acuraccy of the model.
>>>accuracy_score(y_true = y_test, y_pred = predictions)
o/p:0.81534090909090906
It's called accuracy score and if you look at that,
the accuracy of our algorithm here is about 81%.
We can also define the different types of error
in the classification depending of the predicted
and true labels.

------------------------------------------------------------------------------------
CLUSTERING(It is an unsupervised task):
---------------------------------------
A very common application of cluster analysis
that we have discussed before is to divide 
your customer base into segments
based on their purchase histories.Some other 
examples of cluster analysis are;characterization
of different weather patterns for a region,
grouping the latest new articles into topics
to identify the trending topics of the day,
and discovering hot spots for different types of crime
from police reports in order to provide sufficient
police presence for problem areas.

Cluster analysis divides all the samples in a data set
into groups.The goal of cluster analysis is to segment data
so that the differences between samples in the same
cluster are minimized.And the differences between samples of
different clusters are maximized.
Cluster analysis requires some sort of metric
to measure similarity between two samples.
Some common similarity measures are:

A)Euclidean distance, which is the distance along
a straight line between two points, A and B.
B)Manhattan distance, which is calculated on a strictly
horizontal and vertical path,you can only step along 
either the X axis or the Y axis
in the two-dimensional case.
So the path to calculate the Manhattan distance
crosses two segments along the axis,
instead of along a diagonal path
as with the Euclidean distance.
C)Cosine similarity measures the Cosine of the angle
between points A and B.
Since distance measures such as Euclidean distance
are often used to measure similarity between samples
in clustering algorithms, note that it may be necessary
to normalize the input variables so that no one value
dominates the similarity calculation.
As we have learnt about scaling and normalizing earlier.
**Normalizing is one method to scale variables.
Essentially scaling with input variables
puts the variables on the same scale so that all variables
have equal weighting in the calculation to determine
similarity between samples.
Scaling is necessary when you have variables that have
very different scales, such as weight and height.
The magnitude of the weight values which are in pounds,
will be much larger than the magnitude of the height values
which are in feet and inches.
So scaling both variables to a common value range
will make the contributions
from both weight and height equal.

Things to note about cluster analysis are:
first, unlike classification and regression in general,
cluster analysis is an unsupervised task.
This means that there is no target label for any sample
in the data set.
In general, there is no correct clustering results.
The best set of clusters is highly dependent
on the application,
how the resulting clusters will be used.
There are numerical measures to compare
two different clusters,but since there are no
labels to determine whether a sample has been correctly 
clustered,there's no grounds to determine if a
set of clustering results are truly correct or
incorrect.Clusters don't come with labels.You may end
up with five different clusters at the end of cluster 
analysis process, but you don't know what each cluster 
represents.Only by analyzing the samples in each 
cluster further,you can come up with reasonable labels
for your clusters.
***It is important to keep in mind
that interpretation and analysis of the clusters
are required to make sense of and make use of the
results of cluster analysis.
There are several ways that the results of cluster analysis
can be used.
The most obvious is data segmentation
Clusters can also be used to classify new data samples.(scince elements,customers etc based on properties).
Once cluster labels have been determined(say child,young,old groups are labels)
samples in each cluster can be used as label data
for another classification task.
The samples would be the input to the classification model,
and the cluster label will be the target class
for each sample.
Another use of cluster results is
as a basis for anomaly detection.
If a sample is very far away or very different
from any of the cluster centers/groups/segmnts,
then that sample is a cluster outlier and can be flagged
as anomaly.However, these anomalies require further analysis.
Depending on the application,these anomalies can be considered noise
and should be removed from the data set.
An example of this would be a sample with value of 150
for age.
For other cases, these anomalies cases should be studied
more carefully.
Examples of this are in a credit card fraud detection
or network intrusion detection application.
To summarize: cluster analysis is used to organize
similar data items into groups or clusters.
Analyzing the resulting clusters often leads to
useful insights about the characteristics of each group
as well as the underlying structure of the entire data set.
Clusters require analysis and interpretation
to make sense of the results since there are no labels
associated with samples or clusters in a clustering task.

K-Means Clustering:
--------------------
By the end of this topic, you should be able to:
describe the steps in the k-means algorithm,
explain what the k stands for in k-means,
and define what the cluster centroid is.
K-means is a classic algorithm used for cluster analysis.
The algorithm is very simple.
->The first step is to select k initial centroids.(any point in a
graph)( and graph plotted for data of samples for which clutering is being done)
***no. of initial centroid = no. of clusters at last
Therefore K = no. of clusters
A centroid is simply the center of a cluster.
->Next, assign each sample in a data set
to the closest centroid.
This means you calculate the distance
between the sample and each cluster center
and assign the sample to the cluster
with the closest centroid.
->Then you calculate the mean or average of each cluster
to determine a new centroid.
First two steps are done repeatedly
until some stopping criteria is reached.

Note:the final clusters are sensitive to initial centroids.
This means that if cluster results with one set
of initial centroids can be different from results
with another set of initial centroids.

How are initial centroids selected?
There are many approaches to selecting
the initial centroids for k-means
varying in levels of sophistication.
The easiest and most widely used approach
is to apply k-means several times with different
initial centroids randomly chosen to cluster your data set
and then select the centroids that give you
the best clustering results.

To evaluate the cluster results an error measure known
as the Within-Cluster Sum of Squared Error can be used.
The error associated with a sample within a cluster
is the distance between the sample and the cluster centroid.
The squared error for the sample then is the square of that distance.
i.e error = dist b/w sample and centroid
    squared error = error*error
We then do the same thing for all clusters
to get the final calculation
for the Within-Cluster Sum of Squared Error(WSSE)
for all clusters in the results of a cluster analysis run.    
i.e if we have 4 clusters ==> we will have 4 WSSE's
Say if we have two clustering results(WSSE1 and WSSE2),
the one with the smaller WSSE provides the better solution numerically.
i.e if WSSE1<WSSE2 ==> WSSE1 is better
Note :1)This does not cluster set 1 is more correct that cluster set 2
      2)Larger values for K will always reduce WSSE
Therefore, as we've discussed before, there's no ground truth
to mathematically determine which set of clusters
is more correct than the other.
In addition, note that increasing the number of clusters
that is a value for k always reduces the WSSE.
So WSSE should be used with caution.
It only makes sense to use WSSE to compare two sets
of clusters with the same value for k
and generated from the same data set.

There are several methods to determine the value for k
-->Visualization techniques can be used to examine the
data set to see if there are natural groupings of the samples.
Scatterplots and the use of dimensionality reduction
are useful here to visualize the data.
-->A good value for k is application dependent (method).
So domain knowledge of the application can drive
the selection for the value of k.
-->There are also Data-Driven methods
for determining the value of k.
These methods calculate some metric for different values
of k to determine the best selection for k.
One such method is the Elbow method.
The Elbow method for determining the value of K.
If you plot WSSE for different values for k,
we can see how this error measure changes
as the value of k changes as seen in the plot.
The bend in this error curve indicates a drop
in gain by adding more clusters.
This Elbow in the curve provides a suggestion
for a good value of k.
X axis = k(no. of clusters)
Y axs = WSSE results
Note that the elbow cannot
always be unambiguously determined
especially for complex data.
In many cases, the error curve will not have
a clear suggestion for one value but multiple values.
This can be used as a guideline
for the range of values to try for k.

Let's now look at when to stop.
How do you know when to stop iterating
when using k-means?
->One obvious stopping criteria is when
there are no changes to the centroids.
This means that no samples will change cluster assignments,
and re-calculating the centroids
will not result in any changes.
Additional iterations will not bring any more changes
to the cluster results.It's time to stop.
->The second stopping criteria is when the number of 
samples change in clusters is below a certain threshold,
say one percent for example.At this point, the clusters 
are changing only by a few samples resulting in only 
minimal changes to the final clustering results.
The algorithm can be stopped here as well.

Interpreting Results:
Each centroid is the mean of the samples
assigned to that cluster.
You can think of the centroid
as a representative sample for that cluster.
So to interpret the cluster analysis results,
we can examine the cluster centroids.
Comparing the values of the variables between the centroids
will reveal how different or alike clusters are
and provide insight into what each cluster represents.

Live code:Clustering:
-----------------------------------------
We will use cluster analysis to generate a big picture model of the weather at a local station using a minute-graunlarity data. 
In this dataset, we have in the order of millions records. we create 12 clusters our of them.
we will use K-means for clustering

>>>from sklearn.preprocessing import StandardScaler
>>>from sklearn.cluster import KMeans
>>>import utils
>>>import pandas as pd
>>>import numpy as np
>>>from itertools import cycle, islice
>>>import matplotlib.pyplot as plt
>>>from pandas.tools.plotting import parallel_coordinates
>>>%matplotlib inline
Note: utils ws not present ==> installed it using !pip instal utils command and then ran import utils command
>>>data = pd.read_csv('./weather/minute_weather.csv')

Minute Weather Data Description:
The minute weather dataset comes from the same source as the daily weather dataset that we used in the decision tree based classifier notebook. The main difference between these two datasets is that the minute weather dataset contains raw sensor measurements captured at one-minute intervals. Daily weather dataset instead contained processed and well curated data. The data is in the file minute_weather.csv, which is a comma-separated file.
As with the daily weather data, this data comes from a weather station located in San Diego, California. 
The weather station is equipped with sensors that capture weather-related measurements such as air temperature, air pressure, and relative humidity. Data was collected for a period of three years, from September 2011 to September 2014, to ensure that sufficient data for different seasons and weather conditions is captured.

Each row in minute_weather.csv contains weather data captured for a one-minute interval. 
Each row, or sample, consists of the following variables:

rowID: unique number for each row (Unit: NA)
hpwren_timestamp: timestamp of measure (Unit: year-month-day hour:minute:second)
air_pressure: air pressure measured at the timestamp (Unit: hectopascals)
air_temp: air temperature measure at the timestamp (Unit: degrees Fahrenheit)
avg_wind_direction: wind direction averaged over the minute before the timestamp (Unit: degrees, with 0 means coming from the North, and increasing clockwise)
avg_wind_speed: wind speed averaged over the minute before the timestamp (Unit: meters per second)
max_wind_direction: highest wind direction in the minute before the timestamp (Unit: degrees, with 0 being North and increasing clockwise)
max_wind_speed: highest wind speed in the minute before the timestamp (Unit: meters per second)
min_wind_direction: smallest wind direction in the minute before the timestamp (Unit: degrees, with 0 being North and inceasing clockwise)
min_wind_speed: smallest wind speed in the minute before the timestamp (Unit: meters per second)
rain_accumulation: amount of accumulated rain measured at the timestamp (Unit: millimeters)
rain_duration: length of time rain has fallen as measured at the timestamp (Unit: seconds)
relative_humidity: relative humidity measured at the timestamp (Unit: percent)

==>lets examine the data set
>>>data.shape  ==> gives no. of rows and columns
>>>data.head() ==> gives first 5 rows
>>>data[data.isnull().any(axis=1)] ==> displays the Null value roes and gives their count
Data Sampling:
As there are lots of rows,let us sample down by taking every 10th row.
i.e For analysis sake now we sample this data
down to every 10 minutes difference
instead of one minutes.
>>>sampled_df = data[(data['rowID'] % 10) == 0]
>>>sampled_df.shape
==> divided the row id by 10
So sampled_df will have one tenth of the number of rows
in the original data set after we do this.
Stastical analysis:
So let's describe this data set.
>>>sampled_df.describe().transpose()
we are transposing here coz
Recall, if I didn't say transpose ,
the original columns would be displayed as columns.
==> this 'describe' gives all the statstistics like mean,std etc
Lets have a look at the stastictical values now
the mean for air pressure is 916,for
air temperature it is 61 in Fahrenheit,for 
average wind direction it is 162 degrees and things like that.
==>We noticed that some readings are in hundreds,
while others are in decimals.
It's a large difference, especially if you make it
a measure of clustering when you're creating
clustering metrics.

Let's see how many rows have the value zero for columns,
for rain accumulation and rain duration for example.
Rain accumulation is how much it rain and accumulated,
and rain duration is how long it rained for.
>>>sampled_df[sampled_df['rain_accumulation'] == 0].shape
>>>sampled_df[sampled_df['rain_duration'] == 0].shape
==>it looks like a significant number of rows are zero
compared to the size of the data.
So I'd say let's drop these columns,
and also clean any missing values for data cleaning.
So we'll drop the lead sampled_df rain accumulation
and rain duration, because they were mostly zero,
and we also here drop the null values.
>>>del sampled_df['rain_accumulation']
>>>del sampled_df['rain_duration']
>>>rows_before = sampled_df.shape[0]
>>>sampled_df = sampled_df.dropna()
>>>rows_after = sampled_df.shape[0]
>>>rows_before - rows_after
Now let's see what columns/features are left in our
sampled_df data frame.
Out of these features we will now select
a list of features to declare the columns
that we want our algorithm to make use of.
Let's tour those features in a list
that I'll call features here.
==> selecting features fo interest for clustering
>>>features = ['air_pressure', 'air_temp', 'avg_wind_direction', 'avg_wind_speed', 'max_wind_direction', 
>>>        'max_wind_speed','relative_humidity']
So out of about 11 columns we are selecting about seven.
I will use these features to create a data frame
called select data frame.
So remember we had our data,
we put it in the sample data frame,
which was a tenth of our data.
Now we are reducing the data,
the number of columns in the data, not rows,
we get the seven columns that we listed in the feature,
and put it in select_df.
>>>select_df = sampled_df[features]
>>>select_df.columns

Scale the data:
now remember our scalability or scale discussion.
To keep values of different columns comparable,
we scale the values in these features.
So we will use standard scaler sub library for that.
It's an object, and if we give a data frame to it,
(to a function of it in this case)
we'll get a nicely scaled input data
for our clustering operation.
So we have the standard scaler,
and we use the fit_transform function of it,
and we give the select_df.
The fit_transform function here combines
fit and transform operations,
which means it first calculates how much
the data set should be transformed to be scaled.
So it looks at different values and finds
how to scale that.
Then it's going to apply that transformation
to the data frame we give it, in this case select_df,
and it's gonna apply the transformation it came up with
to that data frame.

We will assign the output of this function to Variable(say X)
and that X that we are creating in a nicely scaled fashion
will be an input for our k-Means modeling later.

If we display X we can see nicely scaled values.
they are at the certain scale between a minimum
and a maximum number.So feel free to explore
X further,and compare to the other operations,
other values in the select_df.

>>>X = StandardScaler().fit_transform(select_df)
>>>X

Invoking K-means:
Now we will be invoking the k-Means 
clustering algorithm to create 12 clusters.
==>create a k-Means object called Kmeans,
and give number of clusters for it as 12.
>>>kmeans = KMeans(n_clusters=12)
Now generate the  model by using
the fit operation of that k-Means object,
and input for it is our input data X,
which is the scaled version of the selected data values.
>>>model = kmeans.fit(X)
If you look at the type of this model object,
it is a k-Means object.
>>>print("model\n", model)
o/p:model
    KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
        n_clusters=12, n_init=10, n_jobs=1, precompute_distances='auto',
        random_state=None, tol=0.0001, verbose=0)

==>Once clustering is done, we can look at
each of the cluster centers.
**It will be a list of seven floating point numbers,
which denote where the cluster center stands
in the 7 dimensions of our feature space.
(as we have used 7 features to perform clustering)
>>>centers = model.cluster_centers_
(we are getting the model and assigning
the cluster centers of that model into
a variable called centers)
**cluster_centers_ fn is the attribute of the model.
When we created the model ,it got stored as a part of that model.
>>>centers
(centers(variable) will be an array containing 
12 values,each value is a list containing 7 
values(since 7 features) representing the respective cluster centers)

Now we have the cluster centers for each 
12 clusters, but how do we interact with it?,
how do we visualize it ?
It'll be great to plot these cluster centers
so we can actually examine which clusters
are close to each other and what separates
different clusters from each other.
==>For these here we will create two functions
that help us transform the data
to make it easier for plotting.
Function one here is called pd_centers.
It takes in the cluster centers generated by the model,
the ones that we just displayed in centers,
and creates a Pandas data frame.
and returns that data frame(here P).
Where the header in this dataframe
is the name of the each feature.
It also adds a new column
that denotes the number of the cluster itself.
This new column here is called prediction.
>>># Function that creates a DataFrame with a column for Cluster Number
>>>def pd_centers(featuresUsed, centers):
>>>	colNames = list(featuresUsed)
>>>	colNames.append('prediction')
>>>
>>>	# Zip with a column called 'prediction' (index)
>>>	Z = [np.append(A, index) for index, A in enumerate(centers)]
>>>
>>>	# Convert to pandas data frame for plotting
>>>	P = pd.DataFrame(Z, columns=colNames)
>>>	P['prediction'] = P['prediction'].astype(int)
>>>	return P
So we have the cluster centers,
and the number of the cluster nicely in a data frame,
and we return that as a part of this function.

The second utility function we have here is 
for plotting,and it's called parallel_plot.
It takes in the data for plotting,
which is the data frame that we just generated,
using pd_centers,and generates a colorful graph
with different colors to each cluster.
>>># Function that creates Parallel Plots
>>>
>>>def parallel_plot(data):
>>>	my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(data)))
>>>	plt.figure(figsize=(15,8)).gca().axes.set_ylim([-3,+3])
>>>	parallel_coordinates(data, 'prediction', color = my_colors, marker='o')
Note that this function,also sets the size of 
the plot and controls the range of the Y axis.
i.e We have the figure size and the Y axis limits.
Now create a data frame
>>>P = pd_centers(features, centers)
==>now we can plot using this P
(we will have a parallel plot first)
a parallel coordinates plot is a quick way
to visualize cluster centers along
all the seven dimensions, of our features space,
that's why we created these functions.
>>>parallel_plot(P[P['relative_humidity'] < -0.5])
(above code line gives us the dry days)
i.e gives us the plots for the clusters where avg/scaled value of relative humidity is less than -0.5
and the corrsponding other features for those clusters
==> if rel. humidity is less than -0.5
==> its graph is present in plot and we can see 
other 6 values/features like avg wing speed,temp etc
in the same plot
lly we can get plots for warm day and cold day
>>>parallel_plot(P[P['air_temp'] > 0.5])
(gives warm days details)
>>>parallel_plot(P[(P['relative_humidity'] > 0.5) & (P['air_temp'] < 0.5)])
(for cold day)

------------------------------------------------------------------

                    Regression:
-------------------------------------------
When the model has to predict a numeric value
instead of a category(for classification we predict category),
then the task becomes a regression problem.
An example of regression
is to predict the price of a stock.
The stock price is numeric and not a category,
so this is regression task
instead of a classification task.
Note that if you were to predict
not the actual price of the stock
but whether the stock price will go up or go down,
then that would be a classification task.
That is the main difference
between classification and regression.
In classification you're predicting a category
and in regression,
you're predicting a numeric value.
Recall that in a supervised task,
the target is provided,
while for an unsupervised task,
the target is not available or not known.
Since the target label is provided for each sample,
as a numeric value here,
the regression task is a supervised one
similar to classification.(doubt get clarified)
Since in supervised learning in the training set we hav the values for target
i.e we will have the data which states:"for these inuts we had these outputs"
---------------------
Supervised Learning:
Supervised learning is the machine learning task of inferring 
a function from labeled training data. The training data consist
of a set of training examples. In supervised learning, each example
is a pair consisting of an input object (typically a vector) and a 
desired output value (also called the supervisory signal)
Unsupervised learning:
is a type of machine learning algorithm used to draw inferences
from datasets consisting of input data without labeled responses.
The most common unsupervised learning method is cluster analysis,
which is used for exploratory data analysis to find hidden patterns or grouping in data.
--------------------------
==> Regression is a supervised learning task
==> in case of a supervised learnigntask we will have
a Test data set a Train data set and we built a model and test it.

Training Data:
Used to adjust Model parameters

Test Data:
Evaluate performance on new data

there is also something called Validation data
Validation data:
The validation dataset
is used to determine when to stop the 
training in order to avoid model overfitting.

We we'll discuss linear regression.
Linear Regression:
-------------------
A linear regression model captures the relationships
between a numerical output and the input variables,
and the relationship is modeled as a linear relationship,
hence linear in linear regression.
Eg:
Data set used:Iris flower dataset
This data set has samples of different species
of iris flowers, along with measurements
such as petal width and petal length.

we have a plot with petal width measurements
in centimeters on the x-axis,
and petal length measurements on the y-axis.
Let's say that we want to predict petal length
based on petal width.
==> regression task is:Given a measurement for petal width,
predict the petal length.
y=mx+c(st.line equation (or) linear line equation)
(this line is also ccalled Regression Line)
where c is y intercept(point where the line cuts y-axis)
and m = slope
==>m and c are parameters of model
This regression line can be determined
using the least squares method.

Least Sq. method:
The least squares method finds the regression line
that makes the sum of the residuals as small as possible.
In other words, it finds the line that minimizes
the sum of the squared errors of prediction.
Residuals:suppose a sample lies at a certain 
  distance from Regression line,then the
  The square of this distance is referred to as
  the residual associated with that sample.
  (this distance is the error associated with the sample 
  b/w the value of prediction(reg.Line) and the actual
  value of sample).
The goal of linear regression then,
is to find the best-fitting straight line(Reg.Line)
through the samples using the least squares method.
Once the regression model is built,
we can use it to make predictions.
In linear regression, if there's only one
input variable, then the task is referred to as
simple linear regression.
In cases with more than one input variable, then,
it's referred to as multiple linear regression.
To summarize, the linear regression captures
the linear relationships between a numerical output
and the input variables.
The least squares method can be used to build
a linear regression model by finding
the best-fitting line through the samples.

Live code(Regression):
We now learned regression allows us
to predict continuous variables.
Now, we will use regression technique
to predict a player's overall performance
based on their attributes.

>>>import sqlite3
>>>import pandas as pd 
>>>from sklearn.tree import DecisionTreeRegressor
>>>from sklearn.linear_model import LinearRegression
>>>from sklearn.model_selection import train_test_split
>>>from sklearn.metrics import mean_squared_error
>>>from math import sqrt
#Reading data from SQLite DB into Pandas
#Create your connection.
>>>cnx = sqlite3.connect('database.sqlite')
>>>df = pd.read_sql_query("SELECT * FROM Player_Attributes", cnx)
>>>df.head()
>>>df.shape
o/p : [183978,42]
==> we can see that there are 42 features
Let's declare a list of these features.
now we'll select some of them.
>>>df.columns
We'll select the ones we want to use
for predicting the overall rating.
we select most of them,except the rating.
>>>features = [
          'potential', 'crossing', 'finishing', 'heading_accuracy',
          'short_passing', 'volleys', 'dribbling', 'curve', 'free_kick_accuracy',
          'long_passing', 'ball_control', 'acceleration', 'sprint_speed',
          'agility', 'reactions', 'balance', 'shot_power', 'jumping', 'stamina',
          'strength', 'long_shots', 'aggression', 'interceptions', 'positioning',
          'vision', 'penalties', 'marking', 'standing_tackle', 'sliding_tackle',
          'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning',
          'gk_reflexes']
Lets name the prediction/target as overall rating.
>>>target = ['overall_rating']
Let's now start cleaning the data.
>>>df = df.dropna()
Extract Features and Target ('overall_rating') Values into 2 Separate Dataframes
As we need an x(the input)and a y(output)
to go with the analogy of y=f(x),
with features and target values.
X will be our input data frame.
And we'll select the features we wanted.
We'll load the data frame, without the null values
into X with those features into Y.
>>>X = df[features]
>>>y = df[target]
#look at the data present in X and Y once for a better understanding
#Let us look at a typical row from our features:
>>>X.iloc[2]
#Let us also display our target values:
>>>y
Now lets start with a regression analysis task.
We are doing the same operation using train_test_split.
i.e We are splitting the data set
into test and training sets so we can use one for training
and the rest for testing of the regression algorithm.
#splitting the data
>>>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
We'll perform two different modeling operations
using different regression techniques.
(1)First, we will use a linear regressor.
We'll select the features and use a linear regressor
to predict a player's overall rating.
#Linear Regression: Fit a model to the training set 
>>>regressor = LinearRegression()
>>>regressor.fit(X_train, y_train)
#Perform Prediction using Linear Regression Model
>>>y_prediction = regressor.predict(X_test)
>>>y_prediction
#What is the mean of the expected target value in test set ?
>>>y_test.describe()
#Evaluate Linear Regression Accuracy using Root Mean Square Error
>>>RMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction))
>>>print(RMSE)

